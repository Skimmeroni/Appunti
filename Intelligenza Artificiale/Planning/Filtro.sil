\begin{sile}

	Si consideri una situazione in cui l'agente conosce con
	certezza l'effetto che hanno le sue azioni, ma non conosce
	con certezza lo stato in cui si trova. In ogni istante
	temporale \math{t} l'agente opera una misurazione, che usa
	per fornire una stima probabilistica del trovarsi nello
	stato che si aspetta. In risposta alla misurazione, compie
	una azione e cambia di stato (introducendo ulteriore rumore).

	Si consideri come istante di tempo iniziale \math{t_{0} = 0}.
	Siano:

	\begin{itemize}
		\begin{item}
			\math{x_{t}} lo stato in cui l'agente effettivamente
			si trova allo stato \math{t};
		\end{item}
		\begin{item}
			\math{z_{t}} la misurazione compiuta dall'agente
			all'istante \math{t};
		\end{item}
		\begin{item}
			\math{\mu_{t}} l'azione compiuta dall'agente in
			risposta alla percezione nell'intervallo di tempo
			\math{(t - 1; t]}.
		\end{item}
	\end{itemize}

	\bigskip

	Come giá detto, l'agente non puó conoscere con certezza
	in quale stato si trova, e deve limitarsi a dare una stima
	probabilistica. Sia allora \math{P(x_{t})} la probabilitá
	"in assoluto" che l'agente si trovi nello stato \math{x} al
	tempo \math{t}. É ragionevole assumere che la probabilitá
	che l'agente si trovi in un certo stato in un certo istante
	dipenda in una qualche misura dalle misurazioni e dalle azioni
	compiute in precedenza. In tal senso, ció che si ha interesse
	a calcolare non é tanto \math{P(x_{t})}, quanto quella che
	viene detta \strong{belief} ("fiducia"):

	\begin[width = 50%fw]{parbox}
		\begin[mode = display]{math}
			\mi{bel}(x_{t}) = P(x_{t} | \mu_{1}, z_{1}, \mu_{2}, z_{2},
			\unicodeellipsis, \mu_{t - 1}, z_{t - 1}, \mu_{t}, z_{t})
		\end{math}
	\end{parbox}
	\begin[width = 50%fw]{parbox}
		\begin[mode = display]{math}
			\mi{bel}^{-}(x_{t}) = P(x_{t} | \mu_{1}, z_{1}, \mu_{2}, z_{2},
			\unicodeellipsis, \mu_{t - 1}, z_{t - 1}, \mu_{t})
		\end{math}
	\end{parbox}
	\par

	La funzione a sinistra indica la probabilita che l'agente si
	trovi nello stato \math{x} al tempo \math{t} condizionata da
	tutte le azioni finora intraprese e da tutte le misurazioni
	compiute. La funzione a destra indica la stessa probabilita
	ma \em{prima} che l'ultima misurazione venga effettuata. 

	Si noti come \math{\mu_{t}} e \math{z_{t}} partano da
	\math{t = 1} e non da \math{t = 0}, dato che si assume
	che lo stato \math{x_{0}} venga determinato a priori.
	Ovvero, lo stato iniziale dell'agente deve venirgli
	fornito "dall'esterno", assumendo che questo sia corretto
	con certezza, e poi sulla base di questo aggiorna di volta
	in volta la propria conoscenza sul mondo in termini
	probabilistici.

	Un'assunzione molto forte (e in genere valida) che e possibile
	fare e la cosiddetta \strong{assunzione Markoviana}. In termini
	molto generali, questa prevede che un processo stocastico che
	si evolve nel tempo non dipenda da tutte le osservazioni
	effettuate prima di un un certo istante di tempo, ma solamente
	da quella immediatamente precedente. Nel contesto in esame,
	questo equivale a dire che il grado di fiducia dell'agente al
	tempo \math{t} dipende solamente dalla misurazione compiuta
	e dall'azione intrapresa al tempo \math{t - 1}. Se tale
	assunzione vale, e possibile semplificare le espressioni
	precedenti come:

	\begin[width = 50%fw]{parbox}
		\begin[mode = display]{math}
			\mi{bel}(x_{t}) = P(x_{t} | \mu_{t}, z_{t})
		\end{math}
	\end{parbox}
	\begin[width = 50%fw]{parbox}
		\begin[mode = display]{math}
			\mi{bel}^{-}(x_{t}) = P(x_{t} | \mu_{t})
		\end{math}
	\end{parbox}
	\par

	Al fine di costruire un agente in grado di aggiornare la
	propria conoscenza sul mondo di volta in volta (prendendo
	per buona l'assunzione Markoviana), e necessario che questo
	possa esprimere il proprio grado di fiducia in un certo
	istante in funzione del suo grado di fiducia nell'istante
	precedente \footnote{Questo tipo di approccio viene spesso
	utilizzato nella robotica, dove la posizione di un robot in
	un ambiente ignoto parte da un valore noto e viene mano a
	mano aggiornata ogni qual volta che l'agente si sposta,
	ricalcolando la probabilita che l'agente si trovi nella
	coordinata spaziale che i suoi calcoli rilevano.}.

	\begin{theorem}
		\strong{Filtro Bayesiano per agenti probabilistici}. Se sono
		valide le assunzioni Markoviane, e possibile esprimere il
		grado di fiducia dell'agente al tempo \math{t} in funzione
		del grado di fiducia dell'agente al tempo \math{t - 1} secondo
		la seguente equazione:

		\begin[mode = display]{math}
			bel{(x_{t})} = \eta P{(z_{t} | x_{t})} \int P{(x_{t} |
			\mu_{t}, x_{t - 1})} bel{(x_{t - 1})} d x_{t - 1}
		\end{math}

		\strong{Dimostrazione}. Sia il grado di fiducia dell'agente
		al tempo \math{t} espresso come di consueto:

		\begin[mode = display]{math}
			\mi{bel}(x_{t}) = P(x_{t} | \mu_{1}, z_{1}, \unicodeellipsis, \mu_{t}, z_{t})			
		\end{math}

		Applicando la regola di Bayes:

		\begin[mode = display]{math}
			\mi{bel}(x_{t}) = \eta
			P(z_{t} | x_{t}, \mu_{1}, z_{1}, \unicodeellipsis, \mu_{t})
			P(x_{t} | \mu_{1}, z_{1}, \unicodeellipsis, \mu_{t})
		\end{math}

		Prendendo come valida l'assunzione Markoviana, e possibile
		semplificare l'espressione come:

		\begin[mode = display]{math}
			\mi{bel}(x_{t}) = \eta P(z_{t} | x_{t})
			P(x_{t} | \mu_{1}, z_{1}, \unicodeellipsis, \mu_{t})
		\end{math}

		Applicando la formula delle probabilitá totali:

		\begin[mode = display]{math}
			\mi{bel}(x_{t}) = \eta P(z_{t} | x_{t})
			\int P(x_{t} | \mu_{1}, z_{1}, \unicodeellipsis, \mu_{t}, x_{t - 1})
			P(x_{t - 1} | \mu_{1}, z_{1}, \unicodeellipsis, \mu_{t}) dx_{t - 1}
		\end{math}

		Prendendo come valida l'assunzione Markoviana, e possibile
		semplificare l'espressione come:

		\begin[mode = display]{math}
			\mi{bel}(x_{t}) = \eta P(z_{t} | x_{t}) \int P(x_{t} | \mu_{t}, x_{t - 1})
			P(x_{t - 1} | \mu_{1}, z_{1}, \unicodeellipsis, \mu_{t}) dx_{t - 1}
		\end{math}

		Che equivale a scrivere:

		\begin[mode = display]{math}
			bel{(x_{t})} = \eta P{(z_{t} | x_{t})} \int P{(x_{t} |
			\mu_{t}, x_{t - 1})} bel{(x_{t - 1})} d x_{t - 1}
		\end{math}
	\end{theorem}

	Questa e una forma di filtro Bayesiano. Nel contesto degli
	agenti probabilistici, per implementarlo nella forma di un
	algoritmo, si preferisce separare il procedimento in due
	parti, l'osservazione e l'attuazione.

	L'attuazione corrisponde al ricavare \math{\mi{bel}(x_{t})} a
	partire da \math{\mi{bel}^{-}(x_{t})}. Questo puo essere
	fatto banalmente applicando la regola di Bayes all'espressione
	per \math{\mi{bel}(x_{t})}:

	\begin[mode = display]{math}
		\mi{bel}(x_{t}) = P(x_{t} | \mu_{t}, z_{t}) =
		\eta P(z_{t} | x_{t}) P(x_{t} | \mu_{t}) =
		\eta P(z_{t} | x_{t}) \mi{bel}^{-1}(x_{t})
	\end{math}

	L'osservazione corrisponde al ricavare \math{\mi{bel}^{-}(x_{t})}
	a partire da \math{\mi{bel}(x_{t - 1})}. Questo puo essere fatto
	combinando il risultato appena ottenuto con la formula completa
	per il filtro Bayesiano:

	\begin[mode = display]{math}
		bel{(x_{t})} = \eta P{(z_{t} | x_{t})} \int P{(x_{t} |
		\mu_{t}, x_{t - 1})} bel{(x_{t - 1})} d x_{t - 1} =
		\eta P(z_{t} | x_{t}) \mi{bel}^{-1}(x_{t})
		\thickspace \Rightarrow \thickspace
		\mi{bel}^{-1}(x_{t}) = \int P{(x_{t} |
		\mu_{t}, x_{t - 1})} bel{(x_{t - 1})} d x_{t - 1}
	\end{math}

	Traducendo la ricorsione in una iterazione, si ottiene il
	seguente
	algoritmo:

	\begin{verbatim}
		procedure BAYESIAN-FILTER(bel(X), d)
		    \unichar{U+03B7} \unichar{U+2190} 0
		    if (d e una percezione z) then
		        foreach x in X do
		            bel'(x) \unichar{U+2190} p(z | x) bel(x)
		            \unichar{U+03B7} \unichar{U+2190} \unichar{U+03B7} + bel'(x)
		        foreach x in X do
		            bel'(x) \unichar{U+2190} {\unichar{U+03B7}}\textsuperscript{-1} bel'(x)
		    else if (d e una azione \unichar{U+03B7}) then
		        foreach x in X do
		            bel'(x) \unichar{U+2190} \unichar{U+222B} P(x | \unichar{U+03B7}, x') bel(x') dx'

		    \bigskip 
		    return bel'(X)
	\end{verbatim}

	Si noti come il calcolo di \math{\mi{bel}(x_{t})} debba venire
	ripetuto per tutti i possibili stati \math{x \in X} in ciascuna
	iterazione. Questo puo essere un calcolo estremamente esoso,
	specialmente se il numero di stati sono molti. In genere, il
	procedimento viene ottimizzato escludendo tutti gli stati che
	hanno associata una probabilita molto bassa, ed effettuando
	l'aggiornamento solamente degli stati che hanno probabilita
	oltre una certa soglia minima.

\end{sile}
