\begin{sile}

	Come giá detto, un percettrone non é in grado di operare
	la sua classificazione in presenza di un dataset non
	linearmente separabile. Diventa quindi fondamentale, se
	é possibile, trasformare dataset non linearmente separabili
	in dataset che lo sono. Uno dei possibili approcci prevede
	di introdurre un attributo aggiuntivo agli elementi del
	dataset, di modo che la dimensione dell'iperpiano su cui i
	suoi elementi giacciono aumenti di uno e diventi possibile
	costruire una iperretta che ne separi linearmente gli elementi.

	Questo viene in genere fatto a partire da una trasformazione
	dei valori degli attributi noti. Naturalmente, una trasformazione
	di questo tipo dovrá essere non lineare, altrimenti non si otterrá
	mai un dataset linearmente separabile. Determinare quali e quanti
	attributi sia necessario introdurre nel dataset per compiere questa
	operazione non é un problema banale; inoltre, introdurre nuovi
	attributi nel dataset introduce inevitabilmente anche del rumore.

	Anziché individuare manualmente gli attributi in questione,
	é possibile automatizzare il processo. L'idea é quella di
	"concatenare" piú percettroni probabilistici, dove l'output
	di uno \em{strato} di percettroni fanno da attributi in input
	dello strato successivo. I percettroni piú vicini all'input
	"grezzo" cercano di determinare quali siano gli attributi rilevanti
	per operare la classificazione, mentre quelli piú lontani compiono
	la classificazione in sé. L'output dell'ultimo strato é l'output
	finale dell'insieme di percettroni. Si noti come, stando trattando
	dei percettroni probabilistici, l'output di ciascun percettrone non
	é \math{\pm 1} ma bensí una probabilitá.

	\begin{theorem}
		\strong{Teorema di approssimazione universale}. Una rete
		neurale a due strati costituita da un numero sufficientemente
		grande di neuroni è in grado di approssimare qualsiasi
		funzione continua con accuratezza desiderata (questo non è
		necessariamente un aspetto positivo, dato che rende le reti
		neurali prone all'overfitting).
	\end{theorem}

	Si consideri l'\math{i}-esimo strato della rete neurale. Questa
	sará composta da un certo numero \math{m} di percettroni. Ciascun
	\math{j}-esimo percettrone avrá un proprio output, \math{a_{j}^{i}}.
	Tale output é interamente determinato dai percettroni dello strato
	\math{i - 1}-esimo: in particolare, questo sará dato dall'applicare
	la funzione di attivazione al prodotto scalare fra il vettore dei
	pesi dei percettroni dello strato \math{i - 1}-esimo ed i loro
	valori di output. Assumendo, senza perdita di generalitá, di stare
	usando la funzione sigmoidea come funzione di attivazione, si ha:

	\begin[mode = display]{math}
		a_{j}^{(i)} = \mi{sig} (w_{j, 0} a_{0}^{(i - 1)} + w_{j, 1} a_{1}^{(i - 1)} + \unicodeellipsis + w_{j, n} a_{n}^{(i - 1)})
	\end{math}

	Dove \math{n} é il numero di percettroni del \math{i - 1}-esimo strato,
	che potrebbe essere distinto da \math{m}. Gli output dell'intero strato
	\math{i}-esimo di percettroni é allora dato da:

	\begin[mode = display]{math}
		\table{
			a_{0}^{(i)} = \mi{sig} (w_{0, 0} a_{0}^{(i - 1)} + w_{0, 1} a_{1}^{(i - 1)} + \unicodeellipsis + w_{0, n} a_{n}^{(i - 1)}) \\
			a_{1}^{(i)} = \mi{sig} (w_{1, 0} a_{0}^{(i - 1)} + w_{1, 1} a_{1}^{(i - 1)} + \unicodeellipsis + w_{1, n} a_{n}^{(i - 1)}) \\
			\unicodeellipsis \\
			a_{m}^{(i)} = \mi{sig} (w_{m, 0} a_{0}^{(i - 1)} + w_{m, 1} a_{1}^{(i - 1)} + \unicodeellipsis + w_{m, n} a_{n}^{(i - 1)}) \\
		}
	\end{math}

	Questa struttura puó essere facilmente ricondotta ad una matrice:

	\begin[width = 65%fw]{parbox}
		\begin[mode = display]{math}
			[\table{
				a_{0}^{(i)} \\
				\unicodecdots \\
				a_{m}^{(i)} \\
			}]
			= \mi{sig} {(
			[\table{
				w_{0, 0} & w_{0, 1} & \unicodecdots & w_{0, n} \\
				\vdots   & \ddots   & \vdots        & \vdots   \\
				w_{m, 0} & w_{m, 1} & \unicodecdots & w_{m, n} \\
			}]
			[\table{
				a_{0}^{(i - 1)} \\
				a_{1}^{(i - 1)} \\
				\unicodecdots \\
				a_{n}^{(i - 1)} \\
			}]
			)}
		\end{math}
	\end{parbox}
	\begin[width = 35%fw]{parbox}
		\begin[mode = display]{math}
			{\bi{A}}^{(i)} = \mi{sig} {(\bi{W} {\bi{A}}^{(i - 1)})}
		\end{math}
	\end{parbox}
	\par

	Ci si chiede peró come possa un insieme di percettroni aggiornare
	i vettori dei pesi di ciascuno di questi allo stesso modo in cui
	un singolo percettrone é in grado di farlo. L'approccio piú semplice
	consisterebbe nell'aggiornare di volta in volta i pesi di ciascun
	percettrone, e propagare tale aggiornamento agli strati successivi.
	Un approccio di questo tipo é peró insostenibile, dato che richiederebbe
	un numero di aggiornamenti esponenziale nel numero dei percettroni della
	rete ed una vera rete neurale potrebbe contenerne moltissimi.

	Si potrebbe pensare di aggirare il problema una funzione di euristica
	che permetta di intuire se il cambiamento di un peso davvero influenza
	il peso dei percettroni successivi, ma anche questo é irrealistico. Si
	noti come la difficoltá di questa determinazione derivi dal fatto che
	la funzione di attivazione dei percettroni sia non lineare; se fosse
	lineare, sarebbe garantito che il riassestamento di un percettrone
	influenzi la classificazione degli strati successivi. La non linearitá
	é peró strettamente necessaria, dato che una rete neurale formata da
	percettroni con funzione di attivazione lineare sarebbe a sua volta un
	classificatore lineare, e non avrebbe quindi alcun vantaggio rispetto
	ad un percettrone singolo (sarebbe anzi un inutile spreco di risorse
	computazionali).

	Prende il nome di \strong{likelihood} la probabilitá, dato un modello
	avente parametri \math{\theta}, di osservare un certo insieme di dati
	sulla base di una certa assegnazione di \math{\theta}:

	\begin[mode = display]{math}
		\theta_{ML} = \mi{argmax}_{\theta} P(\bi{X} | \theta)
	\end{math}

	Una volta stabilito il problema, si assume che i dati di tale
	problema siano stocasticamente indipendenti fra loro, ovvero
	che le singole osservazioni non sono influenzate dalle osservazioni
	precedenti. Diviene allora possibile scrivere:

	\begin[mode = display]{math}
		\theta_{ML} =
		\mi{argmax}_{\theta} P(\bi{X} | \theta) =
		\mi{argmax}_{\theta} \Pi_{i} P_{\theta} (X_{i})
	\end{math}

	Si noti peró come gli elementi del dataset da fornire al
	percettrone non abbiano solamente gli attributi \math{\bi{X}}
	ma anche una classificazione (da apprendere). Per questo motivo,
	si preferisce esprimere la likelihood in questa forma:

	\begin[mode = display]{math}
		\theta* =
		\mi{argmax}_{\theta} P(\bi{Y} | \bi{X}, \theta) =
		\mi{argmax}_{\theta} \Pi_{i} P_{\theta} (y_{i} | x_{i})
	\end{math}

	Rispetto al modello di percettrone probabilistico, i parametri
	\math{\theta} corrispondono ai suoi pesi. Sostituendo poi con,
	ad esempio, la funzione softmax, si ha:

	\begin[mode = display]{math}
		l{(w)} = \Pi_{i} \frac{e^{{w_{y}}_{i} \cdot x_{i}}}{\sum_{y} e^{w_{y} \cdot x_{i}}}
	\end{math}

	Per comoditá si preferisce lavorare non sulla likelihood, ma bensí
	sulla \strong{log likelihood}, ovvero sul suo logaritmo naturale:

	\begin[mode = display]{math}
		ll{(w)} = \mi{log} {(l{(w)})} = \sum_{i} \mi{log} P_{w} {(y_{i} | x_{i})} =
		\sum_{i} {w_{y}}_{i} \cdot x_{i} - \mi{log} \sum_{y} e^{w_{y} \cdot x_{i}}
	\end{math}

	Massimizzare tale funzione significa, di fatto, selezionare i
	parametri \math{\bi{w}} che migliorano il trade-off tra le assegnazioni
	di probabilitá a tutti gli elementi del dataset. Aumentando i pesi, gli
	elementi classificati erroneamente avranno dei valori di probabilitá
	bassi, e la (log) likelihood scenderá. Per questo motivo, una maggior
	presenza di rumore richiede che i pesi siano piccoli, in modo tale da
	avere una transizione fra le classificazioni che sia sufficientemente
	uniforme.

	\begin[mode = display]{math}
		\mi{max}_{w} ll{(w)} =
		\mi{max}_{w} \sum_{i} \mi{log} P {(y_{i} | x_{i}, w)} =
		\mi{max}_{w} \sum_{i} \mi{log} \frac{e^{{w_{y}}_{i} \cdot
		f(x_{i})}}{\sum_{y} e^{w_{y} \cdot f(x_{i})}}
	\end{math}

	Spesso, anziché massimizzare la log likelihood, si
	preferisce minimizzare il suo opposto, detto \strong{loss
	function} (\strong{funzione di loss}).

	\begin[mode = display]{math}
		\mi{max}_{w} ll{(w)} = \mi{min}_{w} -ll{(w)}
	\end{math}

	Calcolare questa massimizzazione/minimizzazione significa, di fatto,
	calcolare i punti della funzione in cui il gradiente si annulla. Se
	la funzione ha un numero di dimensioni contenuto ed ha un gradiente
	facilmente calcolabile, come ad esempio nel caso del sigmoide, é
	possibile farlo analiticamente. Se la funzione é multidimensionale,
	questo approccio diviene insostenibile, ed é preferibile calcolare
	i punti di ottimo in maniera approssimata.

	Un algoritmo che permette di farlo prende il nome di \strong{steepest
	descent}, e si basa sull'osservazione che, per raggiungere un punto
	di ottimo a partire da un punto qualsiasi, la direzione che permette
	di raggiungerlo il piú velocemente possibile é quella del gradiente
	valutato nel punto stesso.

	Si consideri, per comoditá, il caso bidimensionale. Indicando con
	\math{\Delta} uno spostamento infinitesimo e con \math{\epsilon}
	una soglia minima, L'intenzione é quella di minimizzare la funzione
	\math{g}:

	\begin[mode = display]{math}
		\mi{min}_{\Delta: \Delta_{1} + \Delta_{2} \leq \epsilon} g(w + \Delta)
	\end{math}

	L'espansione di Taylor al primo ordine di tale funzione é approssimabile a:

	\begin[mode = display]{math}
		g{(w + \Delta)} \approx
		\frac{\partial g}{\partial w_{1}} \Delta_{1} +
		\frac{\partial g}{\partial w_{2}} \Delta_{2}
	\end{math}

	Questo é, di fatto, un prodotto scalare fra due vettori. Ricordando come
	il prodotto scalare abbia valore minimo quando i due vettori hanno verso
	contrario, si ha:

	\begin[mode = display]{math}
		\mi{min}_{\Delta: \Delta_{1} + \Delta_{2} \leq \epsilon} g{(w + \Delta)} \approx
		\mi{min}_{\Delta: \Delta_{1} + \Delta_{2} \leq \epsilon}
		\frac{\partial g}{\partial w_{1}} \Delta_{1} +
		\frac{\partial g}{\partial w_{2}} \Delta_{2} =
		- \nabla {(g)} \frac{\epsilon}{\abs{\abs{\nabla (g)}}} =
		- {[\frac{\partial g}{\partial w_{1}}, \frac{\partial g}{\partial w_{2}}]}^{T}
		\frac{\epsilon}{\abs{\abs{{[\frac{\partial g}{\partial w_{1}},
		\frac{\partial g}{\partial w_{2}}]}^{T}}}}
	\end{math}

	L'idea é quindi quella di partire da un valore qualsiasi per \math{w}
	(anche il vettore nullo) e poi effettuare ripetutamente degli aggiornamenti:

	\begin[mode = display]{math}
		w \leftarrow w - \alpha \nabla {(g{(w)})} = w - \alpha \nabla {(ll{(w)})} =
		w - \alpha \nabla {(\sum_{i = 1}^{m} \mi{log} P{(y_{i} | x_{i}, w)})}
	\end{math}

	Dove \math{\alpha} é un fattore di correzione chiamato \strong{learning rate}.

\end{sile}
