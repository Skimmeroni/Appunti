\begin{sile}

	Il \strong{clustering basato su partizioni} suddivide un
	dataset in insiemi sempre piu piccoli. In questo tipo di
	clustering il dataset viene preso in esame per intero, a
	prescindere da quanto questo sia rumoroso. Questo semplifica
	il procedimento, perché non e necessario alcun preprocessing,
	ma allo stesso tempo rende i cluster molto suscettibili al
	rumore del dataset, in particolare, agli \strong{outlier}, i
	dati isolati molto distanti dal resto;

	\strong{k-means} e un esempio di algoritmo di clustering
	basato su partizioni, che opera su dati esclusivamente
	numerici. L'algoritmo partiziona il dataset fornito in
	\math{k} cluster, con \math{k} fissato. Ciascun cluster
	ha un baricento, chiamato \strong{centroide} (non
	necessariamente un elemento del dataset). L'algoritmo
	é presentato di seguito:

	\begin{enumerate}
		\begin{item}
			Sia scelga un valore \math{k};
		\end{item}
		\begin{item}
			Si scelgano \math{k} elementi qualsiasi a partire
			dal dataset (detti \strong{seed}): questi saranno
			i centroidi iniziali dei \math{k} cluster;
		\end{item}
		\begin{item}
			Per ciascun elemento del dataset che non é un
			centroide, si calcoli la distanza fra tale elemento
			e tutti i centroidi. L'elemento viene assegnato alla
			partizione il cui centroide ha la piú piccola distanza
			da questo. La distanza fra un elemento \math{\bi{x}_{i}}
			ed un centroide \math{\bi{m}_{j}} é data dalla consueta
			formula:

			\begin[mode = display]{math}
				\mi{dist}(\bi{x}_{i}, \bi{m}_{j}) =
				\abs{\abs{\bi{x}_{j} - \bi{m}_{j}}} =
				{({(x_{i, 1} - m_{j, 1})}^{2} + {(x_{i, 2} -
				m_{j, 2})}^{2} + \unicodeellipsis + {(x_{i, r} -
				m_{j, r})}^{2})}^{1 / 2}
			\end{math}
		\end{item}
		\begin{item}
			Si ricalcolino i centroidi sulla base dell'assegnazione
			ai cluster cosí effettuata. Naturalmente, nello spazio
			euclideo, la media di un cluster é data dalla media
			aritmetica dei suoi valori:

			\begin[mode = display]{math}
				\bi{m}_{j} = \frac{1}{\abs{C_{j}}}
				\sum_{\bi{x}_{i} \in C_{j}} \bi{x}_{j}
			\end{math}
		\end{item}
		\begin{item}
			Se é stato raggiunto un criterio di terminazione,
			l'algoritmo termina. Altrimenti, si riprende dal
			punto 3.
		\end{item}
	\end{enumerate}

	\bigskip

	L'algoritmo non specifica un criterio di terminazione. Un
	criterio molto semplice consiste nel fissare un \math{\epsilon}
	e valutare di quanto si discosta il nuovo valore dei centroidi
	(calcolato al punto 4) dal valore precedente: se questo
	scostamento é inferiore ad \math{\epsilon}, l'algoritmo termina.
	Un criterio simile prevede di fissare un \math{\epsilon} e di
	terminare l'algoritmo se il numero di elementi che vengono
	assegnati ad un cluster diverso alla fine della corrente
	iterazione é inferiore ad \math{\epsilon}. Un terzo criterio,
	piu raffinato, e definito a partire dalla funzione \strong{Sum
	of Squared Error}, o \strong{SSE} (\em{Somma degli Errori
	Quadratici}):

	\begin[mode = display]{math}
		\mi{SSE} = \sum_{j = 1}^{k}
		\sum_{\bi{x} \in C_{j}} \mi{dist} {(\bi{x}, \bi{m}_{j})}^{2}
	\end{math}

	Dove \math{C_{j}} indica il \math{j}-esimo cluster,
	\math{\bi{m}_{j}} il centroide del cluster \math{C_{j}}
	e \math{\mi{dist}(\bi{x}, \bi{m}_{j})} la distanza fra
	il punto \math{\bi{x}} ed il centroide \math{\bi{m}_{j}}.
	L'idea e quella di fissare un \math{\epsilon} e fermare
	l'algoritmo quando SSE calcolato nell'iterazione corrente
	si discosta meno di \math{\epsilon} dal valore dell'iterazione
	precedente.

	K-means é efficiente, dato che il suo tempo di esecuzione é
	\math{O(tkn)}, dove \math{k} é il numero di cluster, \math{t}
	é il numero di iterazioni e \math{n} é il numero di elementi
	del dataset; essendo \math{k} fissato e \math{t} (generalmente)
	piccolo, il tempo di esecuzione di k-means é quasi-lineare.

	Idealmente, un cluster é considerabile un buon cluster quando
	ha sia una alta \strong{coesione intra-cluster}, ovvero quando
	é un \strong{cluster compatto} che una alta \strong{coesione
	inter-cluster}, ovvero quando é un \strong{cluster isolato}.
	Un cluster si dice compatto quando é piccola la distanza che
	hanno tutti i punti del cluster dal loro centroide, mentre si
	dice isolato quando é grande la distanza di ogni punto da tutti
	i punti dei cluster diversi dal proprio.

	SSE e un esempio di statistica per la coesione intra-cluster.
	Per quanto riguarda la coesione inter-cluster, si consideri
	un elemento \math{\bi{x}_{i}}, che é stato assegnato al cluster
	\math{C_{I}}. Sia \math{A(\bi{x}_{i})} il valore del centroide
	del cluster a cui \math{\bi{x}_{i}} appartiene, e sia invece
	\math{B(\bi{x}_{i})} la minima distanza media fra \math{\bi{x}_{i}}
	e tutti i punti di \math{D} che non si trovano in \math{C_{I}}.
	Il cluster che ha tale distanza é detto \strong{neighbouring
	cluster}, perché é il cluster in cui sarebbe piú ragionevole
	inserire \math{\bi{x}_{i}} ad eccezione di \math{C_{I}}, essendo
	quello a questo piú vicino.

	\begin[width = 50%fw]{parbox}
		\begin[mode = display]{math}
			A{(\bi{x}_{i})} = \bi{m}_{I} =
			\frac{1}{\abs{C_{I}}}
			\sum_{\bi{x}_{i} \in C_{I}} \bi{x}_{i}
		\end{math}
	\end{parbox}
	\begin[width = 50%fw]{parbox}
		\begin[mode = display]{math}
			B{(\bi{x}_{i})} = \mi{min}_{J \ne I}
			\frac{1}{\abs{C_{J}}} \sum_{\bi{x}_{j} \in C_{J}}
			\mi{dist} {(\bi{x}_{i}, \bi{x}_{j})}
		\end{math}
	\end{parbox}
	\par

	\math{A(\bi{x}_{i})} é una misura di quanto un elemento del
	dataset é vicino al centroide del cluster a cui appartiene,
	mentre \math{B(\bi{x}_{i})} é una misura di quanto un elemento
	del cluster é dissimile dagli elementi degli altri cluster.
	Pertanto, \math{\bi{x}_{i}} si trova in un cluster adatto
	se \math{A(\bi{x}_{i})} é un valore piccolo mentre
	\math{B(\bi{x}_{i})} é un valore grande.

	Prende il nome di \strong{Silhouette} associata a \math{i} la
	quantitá \math{S(\bi{x}_{i})} cosí calcolata:

	\begin[mode = display]{math}
		S{(\bi{x}_{i})} =
		\{\table[columnalign = left left]{
			1 - A(\bi{x}_{i}) / B(\bi{x}_{i}) &
			\mi{se} \thickspace A(\bi{x}_{i}) < B(\bi{x}_{i}) \\
			0 &
			\mi{se} \thickspace A(\bi{x}_{i}) = B(\bi{x}_{i}) \\
			B(\bi{x}_{i}) / A(\bi{x}_{i}) - 1 &
			\mi{se} \thickspace A(\bi{x}_{i}) > B(\bi{x}_{i}) \\
		}
	\end{math}

	É facile verificare che \math{S(\bi{x}_{i})} é un
	valore strettamente compreso fra -1 e 1. Affinché
	\math{S(\bi{x}_{i})} sia vicino ad 1, \math{A(\bi{x}_{i})}
	deve essere un valore piccolo e \math{B(\bi{x}_{i})} deve
	essere un valore grande, pertanto se \math{S(\bi{x}_{i})
	\approx 1} allora \math{\bi{x}_{i}} é stato ben classificato.
	Se invece \math{S(\bi{x}_{i}) \approx -1}, allora
	\math{A(\bi{x}_{i})} é grande e \math{B(\bi{x}_{i})} é
	piccolo, e quindi la classificazione é scadente. Se invece
	\math{S(\bi{x}_{i}) \approx 0}, allora \math{A(\bi{x}_{i})
	\approx B(\bi{x}_{i})}, e quindi l'elemento \math{\bi{x}_{i}}
	potrebbe indifferentemente appartenere al suo cluster o al
	neighbouring cluster.

	Si noti come K-means sia applicabile solamente a dataset i cui
	elementi hanno attributi con valori numerici; se non lo sono,
	occorre codificarli in attributi numerici equivalenti, e non c'e
	garanzia che la semantica possa essere mantenuta. Inoltre, k-means
	é applicabile ai soli dataset sui quali é possibile definire sia
	una distanza che una media fra i suoi elementi.

	Dato che ogni elemento ha lo stesso peso nel computo della
	distanza dai centroidi, la presenza degli outlier destabilizza
	notevolmente il modo in cui i cluster vengono costruiti. Il
	problema puó essere mitigato operando anomaly detection sul
	dataset, prima di operare k-means, di modo da individuare
	quanti piú outlier possibili ed eliminarli. Oppure, in particolar
	modo se il dataset é molto grande e gli outlier non sono (o si
	assume che non siano) molti, é possibile estrarne un sottoinsieme
	mediante random sampling ed applicare k-means su questo, di modo
	da ridurre il piú possibile l'eventualitá che il sottoinsieme
	contenga un outlier.

	Inoltre, il valore di \math{k} non dipende da una qualche
	proprieta intrinseca nel dataset, ma viene scelto arbitrariamente:
	una scelta scadente di \math{k} porta ad un clustering a sua volta
	scadente. Naturalmente e possibile ripetere piu volte l'algoritmo
	usando valori di \math{k} diversi ed osservando quale di questi
	fornisce il clustering piu convincente. Un problema simile e
	quello della scelta dei seed: cambiando seed si puo ottenere un
	clustering completamente diverso. Questo significa che due
	esecuzioni di k-means sullo stesso dataset e con lo stesso
	\math{k} possono dare risultati completamente diversi.

	Infine, il raggruppamento di piú elementi sulla base di una
	distanza genera dei cluster che sono necessariamente delle
	iper-ellissi \math{r}-dimensionali con il centroide al loro
	centro. Tuttavia, non tutti i dataset si adattano a venire
	partizionati in iper-ellissi (o in generale a cluster che sono
	insiemi convessi), ed in questo caso k-means non sará mai in
	grado di fornire dei cluster che ben partizionano tale dataset.

	Nonostante tutti i difetti sopra citati, k-means (e le sue
	varianti) rimane comunque l'algoritmo piú utilizzato per
	risolvere il problema del clustering grazie alla sua
	semplicitá ed alla sua efficienza. Inoltre, non sembrano
	esserci prove che un algoritmo di clustering sia migliore
	degli altri a prescindere dal dataset: in genere, le loro
	performance dipendono dalla forma del dataset e dal tipo
	dei loro attributi.

	\begin{example}
		\begin[width = 25%fw]{parbox}
			\begin[cols = 15%fw 35%fw 35%fw]{ptable}
				\begin{row}
					\cell{\strong{ID}}
					\cell{\strong{X}}
					\cell{\strong{Y}}
				\end{row}
				\begin{row}
					\cell{1} \cell{35.19} \cell{12.189}
				\end{row}
				\begin{row}
					\cell{2} \cell{26.288} \cell{41.718}
				\end{row}
				\begin{row}
					\cell{3} \cell{0.376} \cell{15.506}
				\end{row}
				\begin{row}
					\cell{4} \cell{26.116} \cell{3.963}
				\end{row}
				\begin{row}
					\cell{5} \cell{25.893} \cell{31.515}
				\end{row}
			\end{ptable}
		\end{parbox}
		\begin[width = 25%fw]{parbox}
			\begin[cols = 15%fw 35%fw 35%fw]{ptable}
				\begin{row}
					\cell{\strong{ID}}
					\cell{\strong{X}}
					\cell{\strong{Y}}
				\end{row}
				\begin{row}
					\cell{6} \cell{23.606} \cell{15.402}
				\end{row}
				\begin{row}
					\cell{7} \cell{28.026} \cell{15.47}
				\end{row}
				\begin{row}
					\cell{8} \cell{26.36} \cell{34.488}
				\end{row}
				\begin{row}
					\cell{9} \cell{23.013} \cell{36.213}
				\end{row}
				\begin{row}
					\cell{10} \cell{27.819} \cell{41.867}
				\end{row}
			\end{ptable}
		\end{parbox}
		\begin[width = 25%fw]{parbox}
			\begin[cols = 15%fw 35%fw 35%fw]{ptable}
				\begin{row}
					\cell{\strong{ID}}
					\cell{\strong{X}}
					\cell{\strong{Y}}
				\end{row}
				\begin{row}
					\cell{11} \cell{39.634} \cell{42.23}
				\end{row}
				\begin{row}
					\cell{12} \cell{35.477} \cell{35.104}
				\end{row}
				\begin{row}
					\cell{13} \cell{25.768} \cell{5.967}
				\end{row}
				\begin{row}
					\cell{14} \cell{-0.684} \cell{21.105}
				\end{row}
				\begin{row}
					\cell{15} \cell{3.387} \cell{17.81}
				\end{row}
			\end{ptable}
		\end{parbox}
		\begin[width = 25%fw]{parbox}
			\begin[cols = 15%fw 35%fw 35%fw]{ptable}
				\begin{row}
					\cell{\strong{ID}}
					\cell{\strong{X}}
					\cell{\strong{Y}}
				\end{row}
				\begin{row}
					\cell{16} \cell{32.986} \cell{3.412}
				\end{row}
				\begin{row}
					\cell{17} \cell{34.258} \cell{9.931}
				\end{row}
				\begin{row}
					\cell{18} \cell{6.313} \cell{29.426}
				\end{row}
				\begin{row}
					\cell{19} \cell{33.899} \cell{37.535}
				\end{row}
				\begin{row}
					\cell{20} \cell{4.718} \cell{12.125}
				\end{row}
			\end{ptable}
		\end{parbox}
	\end{example}

\end{sile}
