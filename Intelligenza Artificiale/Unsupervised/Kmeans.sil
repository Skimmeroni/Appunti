\begin{sile}

	\strong{k-means} e un esempio di algoritmo di clustering
	basato su partizioni, che opera su dati esclusivamente
	numerici. L'algoritmo partiziona il dataset fornito in
	\math{k} cluster, con \math{k} fissato. Ciascun cluster
	ha un baricento, chiamato \strong{centroide} (non
	necessariamente un elemento del dataset). L'algoritmo
	é presentato di seguito:

	\begin{enumerate}
		\begin{item}
			Sia scelga un valore \math{k};
		\end{item}
		\begin{item}
			Si scelgano \math{k} elementi qualsiasi a partire
			dal dataset (detti \strong{seed}): questi saranno
			i centroidi iniziali dei \math{k} cluster;
		\end{item}
		\begin{item}
			Per ciascun elemento del dataset che non é un
			centroide, si calcoli la distanza fra tale elemento
			e tutti i centroidi. L'elemento viene assegnato alla
			partizione il cui centroide ha la piú piccola distanza
			da questo. La distanza fra un elemento \math{\bi{x}_{i}}
			ed un centroide \math{\bi{m}_{j}} é data dalla consueta
			formula:

			\begin[mode = display]{math}
				\mi{dist}(\bi{x}_{i}, \bi{m}_{j}) =
				\abs{\abs{\bi{x}_{j} - \bi{m}_{j}}} =
				{({(x_{i, 1} - m_{j, 1})}^{2} + {(x_{i, 2} -
				m_{j, 2})}^{2} + \unicodeellipsis + {(x_{i, r} -
				m_{j, r})}^{2})}^{1 / 2}
			\end{math}
		\end{item}
		\begin{item}
			Si ricalcolino i centroidi sulla base dell'assegnazione
			ai cluster cosí effettuata. Naturalmente, nello spazio
			euclideo, la media di un cluster é data dalla media
			aritmetica dei suoi valori:

			\begin[mode = display]{math}
				\bi{m}_{j} = \frac{1}{\abs{C_{j}}}
				\sum_{\bi{x}_{i} \in C_{j}} \bi{x}_{j}
			\end{math}
		\end{item}
		\begin{item}
			Se é stato raggiunto un criterio di terminazione,
			l'algoritmo termina. Altrimenti, si riprende dal punto 3.
		\end{item}
	\end{enumerate}

	\bigskip

	I criteri di terminazione sono molteplici. Un criterio molto
	semplice consiste nel fissare un certo \math{\epsilon} e valutare
	di quanto si discosta il nuovo valore dei centroidi (calcolato
	al punto 4) dal valore precedente: se questo scostamento é
	inferiore ad \math{\epsilon}, l'algoritmo termina. Oppure,
	similmente, terminare l'algoritmo se il numero di elementi che
	vengono spostati di cluster alla fine della corrente iterazione
	é inferiore ad \math{\epsilon}.

	K-means é indubbiamente molto semplice sia da comprendere
	che da implementare, ma é applicabile solamente a dataset
	con determinate caratteristiche. Innanzitutto, é applicabile
	solamente a dataset i cui elementi hanno esclusivamente
	attributi con valori numerici; se non lo sono, occorre
	preprocessare i dati per convertire gli attributi categoriali
	in attributi numerici equivalenti, e la semantica non puó essere
	sempre mantenuta. Inoltre, k-means é applicabile ai soli dataset
	sui quali é possibile definire sia una distanza che una media fra
	i suoi elementi.

	Inoltre, l'algoritmo é efficiente, dato che il suo tempo di
	esecuzione é \math{O(tkn)}, dove \math{k} é il numero di
	cluster, \math{t} é il numero di iterazioni e \math{n} é il
	numero di elementi del dataset; essendo \math{k} fissato e
	\math{t} (generalmente) piccolo, il tempo di esecuzione di
	k-means é quasi-lineare.

	Dato che ogni elemento ha lo stesso peso nel computo della
	distanza dai centroidi, la presenza degli outlier destabilizza
	notevolmente il modo in cui i cluster vengono costruiti. Il
	problema puó essere mitigato operando anomaly detection sul
	dataset, prima di operare k-means, di modo da individuare
	quanti piú outlier possibili ed eliminarli. Oppure, in particolar
	modo se il dataset é molto grande e gli outlier non sono (o si
	assume che non siano) molti, é possibile estrarne un sottoinsieme
	mediante random sampling ed applicare k-means su questo, di modo
	da ridurre il piú possibile l'eventualitá che il sottoinsieme
	contenga un outlier.

	Infine, il raggruppamento di piú elementi sulla base di una
	distanza genera dei cluster che sono necessariamente delle
	iper-ellissi \math{r}-dimensionali con il centroide al loro
	centro. Tuttavia, non tutti i dataset si adattano a venire
	partizionati in iper-ellissi (o in generale a cluster che sono
	insiemi convessi), ed in questo caso k-means non sará mai in
	grado di fornire dei cluster che ben partizionano tale dataset.

	Nonostante tutti i difetti sopra citati, k-means (e le
	sue varianti) rimane comunque l'algoritmo piú utilizzato
	per risolvere il problema del clustering grazie alla sua
	semplicitá ed alla sua efficienza. Inoltre, non sembrano
	esserci prove che un algoritmo di clustering sia migliore
	degli altri a prescindere dal dataset: in genere, le loro
	performance dipendono dalla forma del dataset e dal tipo
	dei loro attributi.

	\begin{example}
		\begin[width = 25%fw]{parbox}
			\begin[cols = 15%fw 35%fw 35%fw]{ptable}
				\begin{row}
					\cell{\strong{ID}}
					\cell{\strong{X}}
					\cell{\strong{Y}}
				\end{row}
				\begin{row}
					\cell{1} \cell{35.19} \cell{12.189}
				\end{row}
				\begin{row}
					\cell{2} \cell{26.288} \cell{41.718}
				\end{row}
				\begin{row}
					\cell{3} \cell{0.376} \cell{15.506}
				\end{row}
				\begin{row}
					\cell{4} \cell{26.116} \cell{3.963}
				\end{row}
				\begin{row}
					\cell{5} \cell{25.893} \cell{31.515}
				\end{row}
			\end{ptable}
		\end{parbox}
		\begin[width = 25%fw]{parbox}
			\begin[cols = 15%fw 35%fw 35%fw]{ptable}
				\begin{row}
					\cell{\strong{ID}}
					\cell{\strong{X}}
					\cell{\strong{Y}}
				\end{row}
				\begin{row}
					\cell{6} \cell{23.606} \cell{15.402}
				\end{row}
				\begin{row}
					\cell{7} \cell{28.026} \cell{15.47}
				\end{row}
				\begin{row}
					\cell{8} \cell{26.36} \cell{34.488}
				\end{row}
				\begin{row}
					\cell{9} \cell{23.013} \cell{36.213}
				\end{row}
				\begin{row}
					\cell{10} \cell{27.819} \cell{41.867}
				\end{row}
			\end{ptable}
		\end{parbox}
		\begin[width = 25%fw]{parbox}
			\begin[cols = 15%fw 35%fw 35%fw]{ptable}
				\begin{row}
					\cell{\strong{ID}}
					\cell{\strong{X}}
					\cell{\strong{Y}}
				\end{row}
				\begin{row}
					\cell{11} \cell{39.634} \cell{42.23}
				\end{row}
				\begin{row}
					\cell{12} \cell{35.477} \cell{35.104}
				\end{row}
				\begin{row}
					\cell{13} \cell{25.768} \cell{5.967}
				\end{row}
				\begin{row}
					\cell{14} \cell{-0.684} \cell{21.105}
				\end{row}
				\begin{row}
					\cell{15} \cell{3.387} \cell{17.81}
				\end{row}
			\end{ptable}
		\end{parbox}
		\begin[width = 25%fw]{parbox}
			\begin[cols = 15%fw 35%fw 35%fw]{ptable}
				\begin{row}
					\cell{\strong{ID}}
					\cell{\strong{X}}
					\cell{\strong{Y}}
				\end{row}
				\begin{row}
					\cell{16} \cell{32.986} \cell{3.412}
				\end{row}
				\begin{row}
					\cell{17} \cell{34.258} \cell{9.931}
				\end{row}
				\begin{row}
					\cell{18} \cell{6.313} \cell{29.426}
				\end{row}
				\begin{row}
					\cell{19} \cell{33.899} \cell{37.535}
				\end{row}
				\begin{row}
					\cell{20} \cell{4.718} \cell{12.125}
				\end{row}
			\end{ptable}
		\end{parbox}
	\end{example}

\end{sile}
