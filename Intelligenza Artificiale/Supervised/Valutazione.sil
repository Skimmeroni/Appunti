\begin{sile}

	Una volta costruito un modello per risolvere un problema
	di classificazione, si ha interesse a valutarne la qualita,
	testandolo con uno o piu dataset di prova. Questo puo essere
	fatto sia mediante delle metriche \strong{quantitative}, che
	forniscono un numero, che \strong{qualitative}, che forniscono
	una descrizione. Le principali metriche qualitative sono le
	seguenti:

	\begin{itemize}
		\begin{item}
			\strong{Efficienza}, ovvero sia il tempo di esecuzione
			necessario per la costruzione del modello, sia il tempo
			di esecuzione impiegato dal modello nel venire utilizzato;
		\end{item}
		\begin{item}
			\strong{Robustezza}, ovvero quanto bene il modello
			é in grado di gestire dati rumorosi (non cadere
			nell'overfitting) e se é in grado di gestire i dati
			mancanti (e come lo fa);
		\end{item}
		\begin{item}
			\strong{Scalabilitá}, ovvero quanto il modello riesce a
			contenere la crescita al crescere della dimensione dei
			dataset;
		\end{item}
		\begin{item}
			\strong{Interpretabilitá}, ovvero quanto il modello é
			in grado di "spiegare" il suo risultato a chi lo utilizza.
		\end{item}
		\begin{item}
			\strong{Compattezza}, ovvero quanto il modello riesce
			a descrivere il dataset in maniera conservativa (senza 
			componenti ridondanti);
		\end{item}
	\end{itemize}

	\bigskip

	Per trattare le metriche quantitative e prima necessario
	introdurre la seguente matrice, detta \strong{matrice di
	confusione}:

	\begin[cols = 20%fw 40%fw 40%fw, cellborder = 0]{ptable}
		\begin{row}
			\begin{cell}
			\end{cell}
			\begin{cell}
				Classificato come positivo
			\end{cell}
			\begin{cell}
				Classificato come negativo
			\end{cell}
		\end{row}
		\begin{row}
			\begin{cell}
				Effettivamente positivo
			\end{cell}
			\begin{cell}
				TP (\strong{True Positive}): il numero di esempi
				positivi \par classificati correttamente
			\end{cell}
			\begin{cell}
				FN (\strong{False Negative}): il numero di esempi
				positivi \par classificati erroneamente
			\end{cell}
		\end{row}
		\begin{row}
			\begin{cell}
				Effettivamente negativo
			\end{cell}
			\begin{cell}
				FP (\strong{False Positive}): il numero di esempi
				negativi \par classificati erroneamente
			\end{cell}
			\begin{cell}
				TN (\strong{True Negative}): il numero di esempi
				negativi \par classificati correttamente
			\end{cell}
		\end{row}
	\end{ptable}
	\smallskip

	La metrica quantitativa piu intuitiva e l'\strong{accuratezza
	predittiva} \math{A}, ovvero il rapporto fra il numero di
	classificazioni corrette (sia positive che negative) ed il
	numero di elementi del dataset usato per il testing:

	\begin[mode = display]{math}
		A = \frac{TP + TN}{TP + FP + TN + FN} = 
		frac{\mi{Numero} \thickspace \mi{di} \thickspace
		\mi{classificazioni} \thickspace \mi{corrette}}{\mi{Numero}
		\thickspace \mi{totale} \thickspace \mi{di} \thickspace
		\mi{elementi} \thickspace \mi{del} \thickspace \mi{test}
		\thickspace \mi{set}}
	\end{math}

	Naturalmente, l'accuratezza predittiva é tanto maggiore
	quanto piú il suo valore tende ad 1. Oltre a questa,
	sulla base della matrice di confusione e possibile definire
	le metriche \strong{precision} \math{p} (anche detta
	\strong{sensitivity}) e \strong{recall} \math{r}:

	\begin[width = 50%fw]{parbox}
		\begin[mode = display]{math}
			p = \frac{TP}{TP + FP} =
			\frac{\mi{Esempi} \thickspace \mi{positivi}
			\thickspace \mi{classificati} \thickspace
			\mi{correttamente}}{\mi{Esempi} \thickspace
			\mi{classificati} \thickspace \mi{positivi}}
		\end{math}
	\end{parbox}
	\begin[width = 50%fw]{parbox}
		\begin[mode = display]{math}
			r = \frac{TP}{TP + FN} =
			\frac{\mi{Esempi} \thickspace \mi{positivi}
			\thickspace \mi{classificati} \thickspace
			\mi{correttamente}}{\mi{Esempi} \thickspace
			\mi{effettivamente} \thickspace \mi{positivi}}
		\end{math}
	\end{parbox}
	\par

	A differenza dell'accuratezza predittiva, che e piuttosto
	omnicomprensiva, Entrambe le metriche sono focalizzate
	esclusivamente sugli esempi positivi. \math{p} rappresenta
	quanto bene il modello é in grado di classificare correttamente
	gli esempi positivi (tralasciando i falsi negativi), mentre
	\math{r} rappresenta quanto il modello é in grado di "coprire"
	i dati (quanto poco tralascia gli esempi positivi, anche a costo
	di commettere un errore).

	Per comoditá, é possibile combinare le due metriche
	in una sola, chiamata \strong{F\textsubscript{1}-value} (o
	\strong{F\textsubscript{1}-score}), che non é altro che la
	loro media armonica:

	\begin[mode = display]{math}
		F_{1} = {(\frac{p^{-1} + r^{-1}}{2})}^{-1} =
		\frac{2}{p^{-1} + r^{-1}} =
		\frac{2}{\frac{1}{p} + \frac{1}{r}} =
		\frac{2pr}{p + r}
	\end{math}

	Questa metrica é di particolare interesse perché la media
	armonica di due valori tende ad essere vicina al piú piccolo
	dei due. Inoltre, dato che \math{p} e \math{q} compaiono sia
	al numeratore che al denominatore, il valore di \math{F_{1}}
	é grande solamente se sia \math{p} che \math{q} sono a loro
	volta grandi.

	Un'ulteriore metrica e la cosiddetta \strong{specificity}
	\math{s}, definita come il rapporto fra il numero di esempi
	effettivamente negativi ed il numero totale di esempi
	classificati negativi (sia correttamente che non
	correttamente):

	\begin[mode = display]{math}
		s = \frac{TN}{TN + FP} = \frac{\mi{Esempi} \thickspace
		\mi{effettivamente} \thickspace \mi{negativi}}{\mi{Esempi}
		\thickspace \mi{classificati} \thickspace \mi{negativi}}
	\end{math}

	Questa e la "controparte" di \math{p} rispetto agli esempi
	negativi.

	La tecnica piú semplice per allenare un dataset, che e anche
	quella implicitamente utilizzata finora, prende il nome di
	\strong{holdout set}. Questa prevede di separare il dataset
	in due sotto-dataset: uno, chiamato \strong{training set},
	verrá usato per allenare il modello, mentre l'altro, chiamato
	\strong{test set}, verra usato per testarlo. Questa tecnica e
	applicabile quando la dimensione del dataset a disposizione é
	sufficientemente grande da avere due sotto-dataset a loro volta
	grandi. Naturalmente, gli elementi del training set non devono
	figurare nella fase di test, cosi come gli elementi del test
	set non devono figurare nella fase di apprendimento, perche in
	entrambi i casi il modello avrebbe un evidente bias. Separare
	nettamente i due sotto-dataset permette di avere un modello
	funzionante ma che al contempo tollera un minimo margine di
	errore.

	Una tecnica alternativa, chiamata \strong{n-fold
	cross-validation}, é preferibile quando la grandezza
	del dataset a disposizione é limitata. Questa prevede
	di partizionare il dataset a disposizione in \math{m}
	sotto-dataset di uguale grandezza. Per \math{m} volte,
	viene scelto uno degli \math{m} sotto-dataset come test
	set ed i restanti \math{m - 1} sotto-dataset vengono 
	combinati in un training set. A ciascuna iterazione
	della procedura e possibile associare una accuratezza;
	l'accuratezza complessiva viene calcolata come media di
	tutte le \math{m} accuratezze cosí ottenute. In genere,
	sono comuni partizionamenti in 5 (5-fold cross-validation)
	o in 10 (10-fold cross validation) sotto-dataset.

	Nel caso in cui la dimensione del dataset a disposizione
	sia estremamente limitata, é possibile adottare un approccio
	radicale chiamato \strong{leave-one-out cross-validation}, o
	\strong{LOOCV}. L'approccio é di fatto un n-fold cross-validation
	dove la dimensione del test set é unitaria, ovvero se il dataset
	é composto da \math{n} elementi, il sotto-dataset utilizzato per
	la costruzione del modello ha dimensione \math{n - 1} mentre
	quello utilizzato per il testing ha dimensione 1. Come per il
	caso precedente, si ripete l'operazione \math{n} volte e si
	ricava l'accuratezza complessiva a partire dalla media delle
	\math{n} accuratezze parziali.

	Talvolta, trasversalmente a questi approcci, viene introdotto
	un terzo sotto-dataset, chiamato \strong{validation set}.
	Questo viene utilizzato per valutare le prestazioni degli
	\strong{iperparametri}, ovvero i parametri che determinano
	la scelta stessa del tipo di modello. Il modello i cui
	iperparametri sono i piu performanti viene poi allenato con
	il training set e testato con il test set come di consueto.

\end{sile}
