\begin{sile}

	L'\strong{apprendimento mediante alberi di decisione} e un
	esempio di model-based supervised learning pensato per la
	risoluzione di problemi di classificazione. Il modello
	generato da tale apprendimento e, per l'appunto, una
	struttura ad albero chiamato \strong{albero di decisione}.

	%%%
	%%% Esempio preso dal libro, relativo al ristorante, con grafo
	%%%

	Sia i nodi interni che la radice di un albero di decisione sono
	etichettati con gli attributi \math{A_{1}, A_{2}, \unicodeellipsis,
	A_{m}} del dataset \math{D} usato per costruirlo. Da ogni nodo
	interno e dalla radice si diramano tanti archi quanti sono i
	valori che l'attributo del nodo da cui escono puo assumere,
	ciascuno etichettato con uno dei suddetti valori. I nodi foglia
	sono invece etichettati con \tt{True} oppure con \tt{False}. Si
	noti come possano esserci piu noti interni etichettati con lo
	stesso attributo.

	Un percorso che va dalla radice ad un nodo foglia corrisponde
	ad uno degli elementi del dataset usato per costruire l'albero:
	ciascun arco che viene attraversato corrisponde al valore del
	relativo attributo, mentre il nodo foglia indica se l'elemento
	in questione appartiene oppure non appartiene alla classe.
	Quando all'algoritmo viene fornito un dato generico, questo
	percorre l'albero seguendo il percorso descritto dai suoi
	attributi ed emette un verdetto di conseguenza.

	Ciascun percorso che va dalla radice ad un nodo foglia può
	essere tradotto in una formula equivalente nella logica
	proposizionale:

	\begin{center}
		Se attributo1 vale \ddd e attributo2 vale \ddd e \ddd
		e attributo k vale \ddd allora appartiene / non appartiene
		alla classe
	\end{center}

	Formalmente, siano \math{A_{1}, A_{2}, \unicodeellipsis, A_{k}}
	gli attributi che figurano come etichette dei nodi non foglia
	lungo un certo percorso, e siano \math{a_{1}, a_{2},
	\unicodeellipsis, a_{k}} i valori che questi assumono (letti
	lungo gli archi dell'albero). Sia poi \math{b} l'etichetta del
	nodo foglia. Si ha:

	\begin[mode = display]{math}
		a_{1} \wedge a_{2} \wedge \unicodeellipsis \wedge a_{k}
		\rightarrow b
	\end{math}

	Esistono diversi alberi di decisione che rappresentano la stessa
	tabella, alcuni più efficienti di altri. Trovare l'albero di
	decisione \em{migliore} per una tabella, ovvero quello avente
	il minimo numero di nodi, è un problema NP-completo. Esistono
	però algoritmi euristici che permettono di trovare un albero di
	decisione generico (non necessariamente ottimale) con tempo di
	esecuzione approcciabile.

	É possibile costruire un albero di decisione ricorsivamente
	mediante un algoritmo greedy di tipo divide-et-impera. L'albero
	viene costruito in maniera top-down: inizialmente tutti gli
	individui si trovano nel nodo radice, poi vengono distribuiti
	lungo i nodi dell'albero operando delle partizioni.

	\begin{verbatim}
		procedure MAKE-DECISION-TREE(D, A, T)
		1    if (tutti i membri di D appartengono alla stessa classe c\textsubscript{j} in C) then
		2        crea un nodo foglia T avente c\textsubscript{j} come etichetta
		3    else if (A = \unichar{U+2205}) then
		4	    crea un nodo foglia T avente c\textsubscript{j} come etichetta, dove c\textsubscript{j} è la classe avente più membri in D
		5	 else
		6        p0 = ENTROPY(D)
		7        foreach Ai in \{A1, A2, \unichar{U+2026}, An\} do
		8            pi = ENTROPY\textsubscript{Ai}(D)
		9        done
		10       A\textsubscript{g} <= l'attributo in {A1, A2, \unichar{U+2026}, An} che ha il massimo guadagno, ovvero massimo p0 - pi
		11       if (p0 - pi) < threshold then
		12           crea un nodo foglia T avente c\textsubscript{j} come etichetta, dove c\textsubscript{j} è la classe avente più membri in D
		13       else
		14           crea un nodo interno T sulla base di A\textsubscript{g}
		15           partiziona D in v sotto-dataset disgiunti D\textsubscript{1}, D\textsubscript{2}, \unichar{U+2026}, D\textsubscript{v}, dove v sono i valori assumibili da A\textsubscript{g}
		16           foreach Dj in \{D1, D2, \unichar{U+2026}, Dv\} do
		17               if (Dj != \unichar{U+2205}) then
		18                   crea un nodo Tj figlio di T relativo al j-esimo valore assumibile da Ag
		19                   MAKE-DECISION-TREE(Dj, A - \{Ag\}, Tj)
	\end{verbatim}

	L'algoritmo ha in input tre variabili: \tt{D}, \tt{A}
	e \tt{T}. La prima rappresenta l'insieme di elementi
	da classificare nell'iterazione corrente, la seconda
	l'insieme di attributi non ancora analizzati e la terza
	la foglia che viene generata come sostituto al restante
	insieme di elementi a questo livello. Inizialmente, \tt{D}
	corrisponde all'intero dataset, mentre \tt{A} corrisponde
	all'intero insieme di attributi (le etichette delle prime
	\math{n - 1} colonne della tabella); ad ogni iterazione,
	l'insieme \tt{D} viene partizionato, mentre dall'insieme
	\tt{A} viene estratto un attributo che verra analizzato e
	poi rimosso.

	I casi base figurano nelle righe da 1 a 4, e sono due.
	Il primo caso (righe 1 e 2) si verifica quando tutti
	gli elementi di \tt{D} appartengono alla stessa classe:
	in questo caso la classificazione non ha ambiguità, ed
	è sufficiente creare un nodo foglia che ha tale classe
	come etichetta e che contiene l'intero \tt{D}. Il secondo
	caso (righe 3 e 4) si verifica quando \tt{A} e vuoto ma
	non lo e \tt{D}, ovvero quando tutti gli attributi sono
	stati analizzati ma rimangono ancora degli elementi da
	classificare. Anche in questo caso, l'intero \tt{D}
	viene inserito nello stesso nodo foglia ma come etichetta
	viene scelta la classe che contiene il maggior numero di
	rappresentanti in \tt{D}: questa e comunque una approssimazione,
	ma e il miglior compromesso possibile.

	Se non si ricade in nessuno dei due casi base, significa che
	\tt{D} non e vuoto (vi sono ancora degli elementi da classificare)
	ma non lo e nemmeno \tt{A} (vi sono ancora degli attributi da
	considerare). Occorre allora scegliere un attributo sulla base
	del quale eseguire la partizione, rimuovere tale attributo e
	chiamare ricorsivamente l'algoritmo usando come \tt{D} gli
	elementi rimasti e come \tt{A} gli attributi meno quello
	appena esaminato.

	La scelta dell'attributo influisce notevolmente sulla qualita
	dell'albero risultante. L'ideale sarebbe scegliere l'attributo
	che riduce al minimo l'incertezza nel (sott)dataset. A ciascun
	\math{D} è infatti possibile associare una misura di "impurità"
	chiamata \strong{entropia} \footnote{Il termine è associato al
	concetto analogo in fisica.}, che puo essere utilizzata per
	guidare la scelta dell'attributo nell'iterazione corrente.
	Si dice che l'entropia di \tt{D} e alta se \em{quasi} tutti
	i suoi elementi appartengono a classi diverse, mentre si dice
	l'entropia di \tt{D} e bassa se \em{quasi} tutti i suoi elementi
	appartengono alla stessa classe. L'entropia e definita come
	segue:

	\begin[mode = display]{math}
		\mi{entropy} {(D)} =
		- \sum_{j = 1}^{\abs{C}} P{(c_{j})} \mi{log}_{2} {(P{(c_{j})})}
	\end{math}

	Il valore \math{P(c_{j})} indica la probabilitá che, scelto un
	elemento casuale dal dataset \math{D}, questo appartenga alla
	classe \math{c_{j}}. Questo valore, moltiplicato per il logaritmo
	in base due di sé stesso, viene calcolato per ogni classe esistente
	e sommati fra di loro. Essendo \math{P(c_{j}) \in (0, 1)}, si ha
	che \math{\mi{log}(P(c_{j}))} é un numero negativo; questo viene 
	peró reso positivo dal segno meno davanti alla sommatoria. 

	Il prodotto \math{P{(c_{j})} \mi{log}_{2}{(P{(c_{j})})}} é
	complessivamente nullo sia nel caso in cui \math{P(c_{j})
	= 0}, ovvero é certo che non esista alcun elemento di \math{D}
	che appartenga a \math{c_{j}}, sia nel caso in cui \math{P(c_{j})
	= 1}, ovvero é certo che qualsiasi elemento di \math{D} appartiene
	a \math{c_{j}}. Infatti:

	\begin[width = 65%fw]{parbox}
		\begin[mode = display]{math}
			\mi{lim}_{x \rightarrow 0^{+}} P{(c_{j})} \cdot
			\mi{log}_{2} {(P{(c_{j})})} = \mi{lim}_{x \rightarrow 0^{+}}
			\frac{\mi{log}_{2}(P(c_{j}))}{\frac{1}{P(c_{j})}} =
			\mi{lim}_{x \rightarrow 0^{+}} \frac{\frac{1}{\mi{ln}(2)P(c_{j})}}{-
			\frac{1}{P(c_{j})^{2}}} = \mi{lim}_{x \rightarrow 0^{+}}
			\frac{-x}{\mi{ln}(2)} = 0
		\end{math}
	\end{parbox}
	\begin[width = 35%fw]{parbox}
		\begin[mode = display]{math}
			\mi{lim}_{x \rightarrow 1^{-}} P(c_{j}) \cdot
			\mi{log}_{2}(P(c_{j})) = 1 \cdot 0 = 0
		\end{math}
	\end{parbox}
	\par

	Questo significa che il contributo portato dalla classe
	\math{c_{j}} all'entropia complessiva associata a \math{D}
	ha un valore non nullo solamente se la probabilitá che un
	elemento di \math{D} appartenga a \math{c_{j}} é un valore
	che non é né 1 né 0. In altre parole, \math{c_{j}} fa
	aumentare l'entropia associata a \math{D} solamente se un
	elemento di \math{D} \em{potrebbe} appartenere a \math{c_{j}}.
	In particolare, il massimo del contributo all'entropia di
	\math{D} fornito da \math{c_{j}} si ha quando la probabilitá
	che un elemento di \math{D} appartenga a \math{c_{j}} é circa
	un terzo:

	\begin[mode = display]{math}
		\frac{d}{dx} {(P{(c_{j})} \mi{log}_{2} {(P{(c_{j})})})} = 0
		\thickspace\Rightarrow\thickspace
		\mi{log}_{2} {(P{(c_{j})})} + \frac{1}{\mi{ln} {(2)}} = 0
		\thickspace\Rightarrow\thickspace
		\mi{log}_{2} {(P{(c_{j})})} = \frac{-1}{\mi{ln} {(2)}}
		\thickspace\Rightarrow\thickspace
		P{(c_{j})} = 2^{-1/\mi{ln} {(2)}} \approx 0.3679		
	\end{math}

	Se nell'iterazione corrente venisse scelto per compiere
	il partizionamento di \math{D} l'attributo \math{A_{i}},
	che puó assumere \math{v} valori distinti, questo genererá
	\math{v} sottoinsiemi \math{D_{1}, D_{2}, \unicodeellipsis,
	D_{v}}. É possibile calcolare l'entropia che avrebbe \math{D}
	se si scegliesse di partizionarlo sulla base di \math{A_{i}}
	come:

	\begin[mode = display]{math}
		\mi{entropy}_{A_{i}} {(D)} =
		\sum_{j = 1}^{v} \frac{\abs{D_{j}}}{\abs{D}}
		\mi{entropy} {(D_{j})}
	\end{math}

	Dove ciascun termine della sommatoria corrisponde all'entropia
	nel \math{j}-esimo sotto-dataset "pesata" con il rapporto fra
	la sua dimensione e la dimensione dell'intero \math{D}. In questo
	modo, affinché un sotto-dataset contribuisca considerevolmente al
	valore totale dell'entropia di \math{D} dopo la partizione secondo
	\math{A_{i}} deve sia avere una sua alta entropia intrinseca
	sia essere grande (in rapporto all'intero \math{D}).

	Si noti come, a prescindere da quale attributo venga scelto,
	\math{\mi{entropy}_{A_{i}}(D)} sara sempre inferiore a a
	\math{\mi{entropy}(D)}. Questo significa che l'entropia
	di un dataset, mano a mano che i suoi elementi vengono
	partizionati, si riduce sempre di piu.

	La differenza fra l'entropia di \math{D} prima che avvenga
	la partizione (\math{\mi{entropy}(D)}) e l'entropia di
	\math{D} se lo si partizionasse sulla base di \math{A_{i}}
	(\math{\mi{entropy}_{A_{i}} (D)}) indica quanta informazione
	verrebbe "guadagnata" se si scegliesse \math{A_{i}} come
	attributo per il partizionamento:

	\begin[mode = display]{math}
		\mi{gain}(D, A_{i}) = \mi{entropy}(D) - \mi{entropy}_{A_{i}}(D)
	\end{math}

	Questo significa che l'attributo che meglio conviene scegliere
	per partizionare \math{D} in una certa iterazione é quello che
	massimizza il valore di \math{\mi{gain}(D, A_{i})}, sia questo
	\math{A_{g}}. A questo punto (riga 14), \tt{T} diviene un nodo
	interno dell'albero (un nodo di decisione), mentre (riga 15)
	l'insieme \tt{D} viene partizionato in tanti sottoinsiemi
	quanti sono i possibili valori che \math{A_{g}} puo assumere.
	Dopodiche, l'algoritmo, costruisce un sottoalbero \math{T_{j}}
	figlio di \math{T} per ciascun \math{j}-esimo sottoinsieme (riga
	17), per poi richiamare ricorsivamente l'algoritmo per ciascuno
	di questi (righe 18 e 19), passando \math{D_{j}} come primo
	parametro, tutti gli attributi rimasti meno \math{A_{g}} come
	secondo e \math{T_{j}} come terzo.

	Se pero tale guadagno e troppo piccolo, e preferibile operare
	come se non vi fossero piu attributi a disposizione e costruire
	un nodo foglia che contiene tutti gli elementi di \tt{D}
	etichettato con la classe piu rappresentata (righe 11 e 12).
	Questo certamente migliora le prestazioni dell'algoritmo (si
	sacrifica parte della precisione per evitare ulteriori \tt{v}
	chiamate ricorsive) ma soprattutto previene (in parte)
	l'overfitting. Infatti, se il partizionamento venisse fatto
	sempre e comunque, l'albero diverrebbe "troppo preciso" e
	terrebbe conto anche di variazioni infinitesime nei valori
	del dataset, che sono invece meglio spiegate dalla semplice
	presenza di rumore.

	In genere, la presenza di overfitting in un albero di decisione
	e evidente quando l'albero ha troppi livelli e/o dei nodi con
	troppi figli, perché in genere questo si verifica se il dataset
	é molto rumoroso e quindi un certo attributo lo partiziona in
	moltissimi sottoinsiemi ciascuno con pochi rappresentanti.
	Esistono fondamentalmente due approcci per ridurre l'overfitting
	in un albero di decisione, chiamati \strong{pre-pruning} e 
	\strong{post-pruning}.

	Il pre-pruning prevede di bloccare l'espansione di un nodo di
	modo che l'albero non cresca, ed e quello che viene fatto alle
	righe 11 e 12 dell'algoritmo. Il post-pruning opera invece dopo
	che l'albero e stato costruito, cercando e rimuovendo i rami
	superflui. Ad esempio, e possibile scelto un massimo livello
	di profonditá ammissibile e tutto ció che sta al di sotto di
	questa viene unificato, andando per maggioranza.

	Sebbene l'approccio sia stato illustrato per variabili
	discrete, questo puó essere usato anche per la costruzione
	di alberi dove il dataset contiene variabili continue.
	L'idea e quella di suddividere in piu intervalli, non
	necessariamente della stessa lunghezza, i valori che
	questa puo assumere, ed usare ciascun intervallo come
	fosse un valore discreto. Il numero e l'ampiezza di
	tali intervalli deve essere scelta sulla base della
	distribuzione statistica dei valori dell'attributo.

	Essendo un algoritmo divide-et-impera, il tempo
	di esecuzione per la costruzione dell'albero di
	decisione e indicativamente logaritmico. Anche il
	tempo per un'utilizzo dell'albero e indicativamente
	logaritmico, perche ad ogni nodo interno (ad ogni
	decisione) lo spazio di possibilita viene quantomeno
	dimezzato.

\end{sile}
