\begin{sile}

	Gli \strong{alberi di decisione} sono un esempio di model-based
	learning. I dati di partenza su cui allenare il modello sono
	forniti in una rappresentazione tabellare. Sia pertanto data
	una tabella ad \math{n} colonne: le prime \math{n - 1} riportano
	gli attributi di ciascun elemento del dataset, mentre l'ultima
	riporta il valore booleano che determina se tale elemento deve
	venire classificato positivamente (\tt{True}) o negativamente
	(\tt{False}). Gli elementi che come valore dell'ultima colonna
	hanno \tt{True} vengono anche detti \strong{elementi positivi},
	mentre quelli che hanno \tt{False} vengono anche detti
	\strong{elementi negativi}.

	I valori riportati nelle prime \math{n - 1} colonne
	della tabella possono essere sia discreti che continui;
	per semplicitá, si assuma che siano discreti. Gli attributi
	di ciascun elemento della tabella possono essere pensati
	come un vettore multidimensionale ad \math{n - 1}
	componenti. Si indichi con \math{\bi{x}_{j}} il vettore
	\math{(n - 1)}-dimensionale del \math{j}-esimo elemento
	della tabella, con \math{x_{i, j}} l'\math{i}-esimo attributo
	del \math{j}-esimo elemento e con \math{y_{j}} l'ultimo campo
	della tabella per il \math{j}-esimo elemento.

	Un albero di decisione raggiunge la sua conclusione compiendo
	una serie di test, partendo dal nodo radice e seguendo un
	determinato percorso fino a raggiungere un nodo foglia.

	%%%
	%%% Esempio preso dal libro, relativo al ristorante, con grafo
	%%%

	Un albero di decisione è un
	albero in cui, ad ogni nodo, viene presa in esame una delle
	features. Da ciascun nodo si diramano tanti figli quanti sono
	i possibili valori che tale feature può assumere. Nei nodi
	foglia viene scelto il valore dell'annotazione.

	Un percorso che va dalla radice ad un nodo foglia può essere
	tradotto in una \em{regola} ovvero indica quali valori devono
	assumere le features che compaiono lungo i nodi affinchè la
	sequenza di valori porti ad una decisione affermativa (il nodo
	foglia contiene \em{true}) o ad una decisione negativa (il nodo
	foglia contiene \em{false}). Più in generale, un albero di
	decisione booleano è equivalente ad una asserzione logica nella
	forma:

	\begin[mode = display]{math}
		Output \Leftrightarrow (Path_{1} \vee Path_{2} \vee \unicodeellipsis)
	\end{math}

	Dove ciascun \math{Path_{i}} è una congiunzione di coppie
	attributo-valore nella forma \math{A_{m} = v_{x} \wedge
	A_{n} = v_{y} \wedge \unicodeellipsis}, corrispondenti ad
	un percorso dalla radice ad una foglia. Interessante notare
	come tale espressione sia in forma normale a clausole, nello
	specifico nella forma normale disgiunta.

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	Esistono diversi alberi di decisione che rappresentano la stessa
	tabella, alcuni più efficienti di altri. Trovare l'albero di
	decisione \em{migliore} per una tabella, ovvero quello avente
	il minimo numero di nodi, è un problema NP-completo. Esistono
	però algoritmi euristici che permettono di trovare un albero di
	decisione generico (non necessariamente ottimale) con tempo di
	esecuzione approcciabile.

	É possibile costruire un albero di decisione ricorsivamente
	mediante un algoritmo greedy di tipo divide-et-impera. L'albero
	viene costruito in maniera top-down: inizialmente tutti gli
	individui si trovano nel nodo radice, poi vengono distribuiti
	lungo i nodi dell'albero operando delle partizioni.

	\begin{verbatim}
		procedure MAKE-DECISION-TREE(D, A, T)
		1    if (tutti i membri di D appartengono alla stessa classe c\textsubscript{j} in C) then
		2        crea un nodo foglia T avente c\textsubscript{j} come etichetta
		3    else if (A = \unichar{U+2205}) then
		4	    crea un nodo foglia T avente c\textsubscript{j} come etichetta, dove c\textsubscript{j} è la classe avente più membri in D
		5	 else
		6        p0 = ENTROPY(D)
		7        foreach Ai in \{A1, A2, \unichar{U+2026}, An\} do
		8            pi = ENTROPY\textsubscript{Ai}(D)
		9        done
		10       A\textsubscript{g} <= l'attributo in {A1, A2, \unichar{U+2026}, An} che ha il massimo guadagno, ovvero massimo p0 - pi
		11       if (p0 - pi) < threshold then
		12           crea un nodo foglia T avente c\textsubscript{j} come etichetta, dove c\textsubscript{j} è la classe avente più membri in D
		13       else
		14           crea un nodo interno T sulla base di A\textsubscript{g}
		15           partiziona D in v sotto-dataset disgiunti D\textsubscript{1}, D\textsubscript{2}, \unichar{U+2026}, D\textsubscript{v}, dove v sono i valori assumibili da A\textsubscript{g}
		16           foreach Dj in \{D1, D2, \unichar{U+2026}, Dv\} do
		17               if (Dj != \unichar{U+2205}) then
		18                   crea un nodo Tj figlio di T relativo al j-esimo valore assumibile da Ag
		19                   MAKE-DECISION-TREE(Dj, A - \{Ag\}, Tj)
	\end{verbatim}

	L'algoritmo ha in input tre variabili: \tt{D}, \tt{A}
	e \tt{T}. La prima rappresenta l'insieme di elementi
	da classificare nell'iterazione corrente, la seconda
	l'insieme di attributi non ancora analizzati e la terza
	la foglia che viene generata come sostituto al restante
	insieme di elementi a questo livello. Inizialmente, \tt{D}
	corrisponde all'intero dataset, mentre \tt{A} corrisponde
	all'intero insieme di attributi (le etichette delle prime
	\math{n - 1} colonne della tabella); ad ogni iterazione,
	l'insieme \tt{D} viene partizionato, mentre dall'insieme
	\tt{A} viene estratto un attributo che verra analizzato e
	poi rimosso.

	I casi base figurano nelle righe da 1 a 4, e sono due.
	Il primo caso (righe 1 e 2) si verifica quando tutti
	gli elementi di \tt{D} appartengono alla stessa classe:
	in questo caso la classificazione non ha ambiguità, ed
	è sufficiente creare un nodo foglia che ha tale classe
	come etichetta e che contiene l'intero \tt{D}. Il secondo
	caso (righe 3 e 4) si verifica quando \tt{A} e vuoto ma
	non lo e \tt{D}, ovvero quando tutti gli attributi sono
	stati analizzati ma rimangono ancora degli elementi da
	classificare. Anche in questo caso, l'intero \tt{D}
	viene inserito nello stesso nodo foglia ma come etichetta
	viene scelta la classe che contiene il maggior numero di
	rappresentanti in \tt{D}: questa e comunque una approssimazione,
	ma e il miglior compromesso possibile.

	Se non si ricade in nessuno dei due casi base, significa che
	\tt{D} non e vuoto (vi sono ancora degli elementi da classificare)
	ma non lo e nemmeno \tt{A} (vi sono ancora degli attributi da
	considerare). Occorre allora scegliere un attributo sulla base
	del quale eseguire la partizione, rimuovere tale attributo e
	chiamare ricorsivamente l'algoritmo usando come \tt{D} gli
	elementi rimasti e come \tt{A} gli attributi meno quello
	appena esaminato.

	La scelta dell'attributo influisce notevolmente sulla qualita
	dell'albero risultante. L'ideale sarebbe scegliere l'attributo
	che riduce al minimo l'incertezza nel (sott)dataset. A ciascun
	\math{D} è infatti possibile associare una misura di "impurità"
	chiamata \strong{entropia} \footnote{Il termine è associato al
	concetto analogo in fisica.}, che puo essere utilizzata per
	guidare la scelta dell'attributo nell'iterazione corrente.
	Si dice che l'entropia di \tt{D} e alta se \em{quasi} tutti
	i suoi elementi appartengono a classi diverse, mentre si dice
	l'entropia di \tt{D} e bassa se \em{quasi} tutti i suoi elementi
	appartengono alla stessa classe. L'entropia e definita come
	segue:

	\begin[mode = display]{math}
		\mi{entropy} {(D)} =
		- \sum_{j = 1}^{\abs{C}} P{(c_{j})} \mi{log}_{2} {(P{(c_{j})})}
	\end{math}

	Il valore \math{P(c_{j})} indica la probabilitá che, scelto un
	elemento casuale dal dataset \math{D}, questo appartenga alla
	classe \math{c_{j}}. Questo valore, moltiplicato per il logaritmo
	in base due di sé stesso, viene calcolato per ogni classe esistente
	e sommati fra di loro. Essendo \math{P(c_{j}) \in (0, 1)}, si ha
	che \math{\mi{log}(P(c_{j}))} é un numero negativo; questo viene 
	peró reso positivo dal segno meno davanti alla sommatoria. 

	Il prodotto \math{P{(c_{j})} \mi{log}_{2}{(P{(c_{j})})}} é
	complessivamente nullo sia nel caso in cui \math{P(c_{j})
	= 0}, ovvero é certo che non esista alcun elemento di \math{D}
	che appartenga a \math{c_{j}}, sia nel caso in cui \math{P(c_{j})
	= 1}, ovvero é certo che qualsiasi elemento di \math{D} appartiene
	a \math{c_{j}}. Infatti:

	\begin[width = 65%fw]{parbox}
		\begin[mode = display]{math}
			\mi{lim}_{x \rightarrow 0^{+}} P(c_{j}) \cdot
			\mi{log}_{2}(P(c_{j})) = \mi{lim}_{x \rightarrow 0^{+}}
			\frac{\mi{log}_{2}(P(c_{j}))}{\frac{1}{P(c_{j})}} =
			\mi{lim}_{x \rightarrow 0^{+}} \frac{\frac{1}{\mi{ln}(2)P(c_{j})}}{-
			\frac{1}{P(c_{j})^{2}}} = \mi{lim}_{x \rightarrow 0^{+}}
			\frac{-x}{\mi{ln}(2)} = 0
		\end{math}
	\end{parbox}
	\begin[width = 35%fw]{parbox}
		\begin[mode = display]{math}
			\mi{lim}_{x \rightarrow 1^{-}} P(c_{j}) \cdot
			\mi{log}_{2}(P(c_{j})) = 1 \cdot 0 = 0
		\end{math}
	\end{parbox}
	\par

	Questo significa che il contributo portato dalla classe
	\math{c_{j}} all'entropia complessiva associata a \math{D}
	ha un valore non nullo solamente se la probabilitá che un
	elemento di \math{D} appartenga a \math{c_{j}} é un valore
	che non é né 1 né 0. In altre parole, \math{c_{j}} fa
	aumentare l'entropia associata a \math{D} solamente se un
	elemento di \math{D} \em{potrebbe} appartenere a \math{c_{j}}.
	In particolare, il massimo del contributo all'entropia di
	\math{D} fornito da \math{c_{j}} si ha quando la probabilitá
	che un elemento di \math{D} appartenga a \math{c_{j}} é circa
	un terzo:

	\begin[mode = display]{math}
		\frac{d}{dx} {(P{(c_{j})} \mi{log}_{2} {(P{(c_{j})})})} = 0
		\thickspace\Rightarrow\thickspace
		\mi{log}_{2} {(P{(c_{j})})} + \frac{1}{\mi{ln} {(2)}} = 0
		\thickspace\Rightarrow\thickspace
		\mi{log}_{2} {(P{(c_{j})})} = \frac{-1}{\mi{ln} {(2)}}
		\thickspace\Rightarrow\thickspace
		P{(c_{j})} = 2^{-1/\mi{ln} {(2)}} \approx 0.3679		
	\end{math}

	Se nell'iterazione corrente venisse scelto per compiere
	il partizionamento di \math{D} l'attributo \math{A_{i}},
	che puó assumere \math{v} valori distinti, questo genererá
	\math{v} sottoinsiemi \math{D_{1}, D_{2}, \unicodeellipsis,
	D_{v}}. É possibile calcolare l'entropia che avrebbe \math{D}
	se si scegliesse di partizionarlo sulla base di \math{A_{i}}
	come:

	\begin[mode = display]{math}
		\mi{entropy}_{A_{i}} {(D)} =
		\sum_{j = 1}^{v} \frac{\abs{D_{j}}}{\abs{D}}
		\mi{entropy} {(D_{j})}
	\end{math}

	Dove ciascun termine della sommatoria corrisponde all'entropia
	nel \math{j}-esimo sotto-dataset "pesata" con il rapporto fra
	la sua dimensione e la dimensione dell'intero \math{D}. In questo
	modo, affinché un sotto-dataset contribuisca considerevolmente al
	valore totale dell'entropia di \math{D} dopo la partizione secondo
	\math{A_{i}} deve sia avere una sua alta entropia intrinseca
	sia essere grande (in rapporto all'intero \math{D}).

	Si noti come, a prescindere da quale attributo venga scelto,
	\math{\mi{entropy}_{A_{i}}(D)} sara sempre inferiore a a
	\math{\mi{entropy}(D)}. Questo significa che l'entropia
	di un dataset, mano a mano che i suoi elementi vengono
	partizionati, si riduce sempre di piu.

	La differenza fra l'entropia di \math{D} prima che avvenga
	la partizione (\math{\mi{entropy}(D)}) e l'entropia di
	\math{D} se lo si partizionasse sulla base di \math{A_{i}}
	(\math{\mi{entropy}_{A_{i}} (D)}) indica quanta informazione
	verrebbe "guadagnata" se si scegliesse \math{A_{i}} come
	attributo per il partizionamento:

	\begin[mode = display]{math}
		\mi{gain}(D, A_{i}) = \mi{entropy}(D) - \mi{entropy}_{A_{i}}(D)
	\end{math}

	Questo significa che l'attributo che meglio conviene scegliere
	per partizionare \math{D} in una certa iterazione é quello che
	massimizza il valore di \math{\mi{gain}(D, A_{i})}, sia questo
	\math{A_{g}}. A questo punto (riga 14), \tt{T} diviene un nodo
	interno dell'albero (un nodo di decisione), mentre (riga 15)
	l'insieme \tt{D} viene partizionato in tanti sottoinsiemi
	quanti sono i possibili valori che \math{A_{g}} puo assumere.
	Dopodiche, l'algoritmo, costruisce un sottoalbero \math{T_{j}}
	figlio di \math{T} per ciascun \math{j}-esimo sottoinsieme (riga
	17), per poi richiamare ricorsivamente l'algoritmo per ciascuno
	di questi (righe 18 e 19), passando \math{D_{j}} come primo
	parametro, tutti gli attributi rimasti meno \math{A_{g}} come
	secondo e \math{T_{j}} come terzo.

	Se pero tale guadagno e troppo piccolo, e preferibile operare
	come se non vi fossero piu attributi a disposizione e costruire
	un nodo foglia che contiene tutti gli elementi di \tt{D}
	etichettato con la classe piu rappresentata (righe 11 e 12).
	Questo certamente migliora le prestazioni dell'algoritmo (si
	sacrifica parte della precisione per evitare ulteriori \tt{v}
	chiamate ricorsive) ma soprattutto previene (in parte)
	l'overfitting. Infatti, se il partizionamento venisse fatto
	sempre e comunque, l'albero diverrebbe "troppo preciso" e
	terrebbe conto anche di variazioni infinitesime nei valori
	del dataset, che sono invece meglio spiegate dalla semplice
	presenza di rumore.

	In genere, la presenza di overfitting in un albero di decisione
	e evidente quando l'albero ha troppi livelli e/o dei nodi con
	troppi figli, perché in genere questo si verifica se il dataset
	é molto rumoroso e quindi un certo attributo lo partiziona in
	moltissimi sottoinsiemi ciascuno con pochi rappresentanti.
	Esistono fondamentalmente due approcci per ridurre l'overfitting
	in un albero di decisione, chiamati \strong{pre-pruning} e 
	\strong{post-pruning}.

	Il pre-pruning prevede di bloccare l'espansione di un nodo di
	modo che l'albero non cresca, ed e quello che viene fatto alle
	righe 11 e 12 dell'algoritmo. Il post-pruning opera invece dopo
	che l'albero e stato costruito, cercando e rimuovendo i rami
	superflui. Ad esempio, e possibile scelto un massimo livello
	di profonditá ammissibile e tutto ció che sta al di sotto di
	questa viene unificato, andando per maggioranza.

	Sebbene l'approccio sia stato illustrato per variabili
	discrete, questo puó essere usato anche per la costruzione
	di alberi dove il dataset contiene variabili continue.
	L'idea e quella di suddividere in piu intervalli, non
	necessariamente della stessa lunghezza, i valori che
	questa puo assumere, ed usare ciascun intervallo come
	fosse un valore discreto. Il numero e l'ampiezza di
	tali intervalli deve essere scelta sulla base della
	distribuzione statistica dei valori dell'attributo.

	Essendo un algoritmo divide-et-impera, il tempo
	di esecuzione per la costruzione dell'albero di
	decisione e indicativamente logaritmico. Anche il
	tempo per un'utilizzo dell'albero e indicativamente
	logaritmico, perche ad ogni nodo interno (ad ogni
	decisione) lo spazio di possibilita viene quantomeno
	dimezzato.

\end{sile}
