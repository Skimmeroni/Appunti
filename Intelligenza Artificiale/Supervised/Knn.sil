\begin{sile}

	\strong{K-nearest neighbour} (\strong{kNN}) e una tecnica
	di individual-based learning che permette di classificare
	un dataset con il solo requisito di avere a disposizione
	una metrica per definire una distanza fra gli elementi di
	un dataset.

	Si assuma di avere a disposizione un dataset \math{D} giá
	parzialmente classificato. Fissato un numero di vicini
	\math{k} e scelta una metrica opportuna per gli elementi
	di \math{D}, l'algoritmo procede come segue:

	\begin{enumerate}
		\begin{item}
			Preso un elemento \math{d \in D} non ancora classificato,
			si calcoli la distanza fra \math{d} e tutti gli altri
			elementi di \math{D};
		\end{item}
		\begin{item}
			Si costruisca l'insieme \math{N \subseteq D} formato
			dai \math{k} elementi di \math{D} che hanno la piú
			piccola distanza da \math{d};
		\end{item}
		\begin{item}
			Sia \math{c} la classe che figura piú spesso fra
			gli elementi di \math{N}. Se si verifica un conflitto,
			ovvero se esistono due o piu classi che figurano piu
			di tutte le altre in \math{N} ma che hanno fra loro
			la stessa cardinalita, \math{c} viene scelta come
			una qualsiasi di queste. All'elemento \math{d} viene
			assegnata la classe \math{c};
		\end{item}
		\begin{item}
			Se esiste ancora almeno un elemento \math{d' \in D}
			non classificato, l'algoritmo riparte considerando
			\math{d'}. Altrimenti, l'algoritmo termina.
		\end{item}
	\end{enumerate}
	\bigskip

	L'idea dell'algoritmo é di stimare \math{P(c | d)}, ovvero
	la probabilitá che la classificazione corretta sia scegliere
	la classe \math{c} dato l'individuo \math{d}, con \math{\abs{P}/k},
	ovvero il rapporto fra il numero di vicini di \math{d} con piú
	rappresentanti ed il numero totale di vicini di \math{d}. Essendo
	\math{N} un insieme estratto dai \math{k} vicini di \math{d},
	il valore \math{\abs{N}/k} é certamente compreso fra 0 e 1, ed
	é quindi un valore di probabilitá.

	Il valore di \math{k} viene in genere scelto in maniera
	empirica, operando ad esempio un \math{k}-fold cross
	validation ed osservando quale valore di \math{k} rende
	i risultati migliori. É preferibile scegliere un numero
	dispari come valore di \math{k}, perché in questo modo é
	piú raro che possa verificarsi una situazione di conflitto.

	La funzione di distanza dipende dal dominio del dataset
	in esame. Nel caso limite in cui gli attributi del dataset
	siano valori numerici e vada loro assegnato lo stesso peso,
	e possibile utilizzare la \strong{distanza di Minkowski}.
	Indicando con \math{p} e il numero di attributi del dataset
	e con \math{\bi{d}_{q}} l'elemento sul quale l'algoritmo kNN
	sta operando, la distanza di Minkowski fra \math{\bi{d}_{q}}
	ed un altro elemento \math{\bi{d}_{j} \in D} é data da:

	\begin[mode = display]{math}
		L^{p} {(\bi{d}_{q}, \bi{d}_{j})} =
		{(\sum_{i} {(\abs{\bi{d}_{q, i} - \bi{d}_{j, i}})}^{p})}^{1/p}
	\end{math}

	Dove \math{\bi{d}_{j, i}} indica l'\math{i}-esimo attributo
	dell'elemento \math{\bi{d}_{j}}.

	Non dovendo costruire alcun modello, il tempo di induzione
	di k-nearest neighbour é, tecnicamente, nullo. D'altro canto,
	in ciascuna esecuzione dell'algoritmo occorre calcolare la
	distanza fra ciascun elemento del dataset e tutti gli altri
	elementi: anche applicando tecniche di programmazione dinamica,
	non e possibile scendere sotto un tempo di esecuzione lineare
	nella dimensione del dataset.

	Si noti come non su tutti i domini sia possibile definire una
	nozione di distanza, pertanto in tali situazioni K-nearest
	neighbour e inapplicabile. Inoltre, anche avendo una distanza
	a disposizione, il semplice fatto che degli elementi del dataset
	siano vicini fra loro non implica necessariamente che esista fra
	loro una qualche correlazione semantica.

	Per questi e altri motivi, kNN ha una applicabilitá piú limitata
	rispetto, ad esempio, agli alberi di decisione, che richiedono
	molte meno assunzioni. Inoltre, quando il numero di dimensioni é
	grande, diventa molto difficile trovare dei punti che siano vicini
	fra di loro. Nonostante questo, le prestazioni di kNN sono comunque
	competitive, addirittura superando, in certe situazioni, algoritmi
	molto piú elaborati.

\end{sile}
