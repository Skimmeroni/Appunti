\begin{sile}
		
	Nel \strong{supervised learning} l'agente sviluppa
	modelli predittivi sia sulla base dell'input che
	dell'output. Nello specifico, osserva diverse coppie
	di input-output e cerca di determinare la funzione che
	meglio mappa ogni input al relativo output. Un esempio
	di supervised learning é la \strong{classificazione}:
	ai dati in input viene associata una \strong{label} ed
	i dati vengono raggruppati sulla base di tali label.

	L'obiettivo del supervised learning é il seguente: dato
	un \strong{training set} di \math{N} esempi, costituiti
	da coppie input-output \math{(x_{1}, y_{1}), (x_{2}, y_{2}),
	\unicodeellipsis, (x_{n}, y_{n})} generati da una funzione
	ignota \math{y = f(x)}, si trovi la funzione \math{h} che
	meglio approssima \math{f}. Si noti come \math{x_{i}} possa
	essere indifferentemente uno scalare oppure un vettore a
	\math{k} componenti.

	La funzione \math{h} viene chiamata \strong{ipotesi}, ed
	é estratta da uno \strong{spazio di ipotesi} \math{H} di
	possibili funzioni. Con un diverso vocabolario é possibile
	chiamare \math{h} il \strong{modello} dei dati, estratto da
	una \strong{classe di modelli} \math{H}, oppure come una
	\strong{funzione} estratta da una \strong{classe di funzioni}.
	I valori dell'output \math{y_{i}} vengono chiamati \strong{veritá
	di base}: rappresentano i valori "reali" che il modello deve
	cercare di prevedere. In prima battuta, é possibile considerare
	buona un'ipotesi \math{h} se, per ogni coppia input-output
	\math{(x_{i}, y_{i})}, \math{h(x_{i})} restituisce un valore
	che approssima bene \math{y_{i}}.

	\begin{example}
		Un esempio di supervised learning si ha nei filtri antispam
		dei client di posta elettronica. L'idea é quella di fornire
		al filtro un grande quantitativo di email contrassegnate come
		spam ed un grande quantitativo di email contrassegnate come
		non spam, di modo che questo possa estrarre dei pattern comuni
		nelle email spam e non spam. A questo punto, se viene fornita
		al filtro una mail qualsiasi, questo é (dovrebbe essere) in
		grado di determinare autonomamente se la mail é o non é spam. 
	\end{example}

	Vi sono due situazioni patologiche molto comuni che possono
	presentarsi nella costruzione di un modello supervisionato,
	\strong{underfitting} e \strong{overfitting}. La prima si 
	verifica quando il modello costruito predice male i dati
	passati, mentre la seconda si verifica quando il modello
	predice "cosi bene" i dati su cui e allenato da non tollerare
	alcun grado di impurita, tanto da fallire per molti dataset
	solo leggermente differenti Naturalmente, la prima situazione
	e peggiore della seconda, perche se l'overfitting produce
	un modello scadente ma comunque funzionante, l'underfitting
	produce un modello del tutto inutilizzabile.

	Esistono sia algoritmi che costruiscono un modello, e che 
	quindi distinguono fra fase di training e fase di test, 
	chiamati \strong{model-based learning}, sia algoritmi che
	non lo costruiscono, dove tutto e insieme, chiamati
	\strong{individual-based learning}.

	Sia data una tabella avente \math{n} colonne, e le prime
	\math{n - 1} sono le features, mentre l'ultima è l'annotazione.
	Se tale annotazione è un valore booleano, si parla di
	\strong{classificazione}, mentre se è un valore numerico
	si parla di \strong{regressione}.

	La tecnica piú semplice per allenare un dataset, che e anche
	quella implicitamente utilizzata finora, prende il nome di
	\strong{holdout set}. Questa prevede di separare il dataset
	in due sotto-dataset: uno, chiamato \strong{training set},
	verrá usato per allenare il modello, mentre l'altro, chiamato
	\strong{test set}, verra usato per testarlo. Questa tecnica e
	applicabile quando la dimensione del dataset a disposizione é
	sufficientemente grande da avere due sotto-dataset a loro volta
	grandi. Naturalmente, gli elementi del training set non devono
	figurare nella fase di test, cosi come gli elementi del test
	set non devono figurare nella fase di apprendimento, perche in
	entrambi i casi il modello avrebbe un evidente bias. Separare
	nettamente i due sotto-dataset permette di avere un modello
	funzionante ma che al contempo tollera un minimo margine di
	errore.

	Una tecnica alternativa, chiamata \strong{n-fold
	cross-validation}, é preferibile quando la grandezza
	del dataset a disposizione é limitata. Questa prevede
	di partizionare il dataset a disposizione in \math{m}
	sotto-dataset di uguale grandezza. Per \math{m} volte,
	viene scelto uno degli \math{m} sotto-dataset come test
	set ed i restanti \math{m - 1} sotto-dataset vengono 
	combinati in un training set. A ciascuna iterazione
	della procedura e possibile associare una accuratezza;
	l'accuratezza complessiva viene calcolata come media di
	tutte le \math{m} accuratezze cosí ottenute. In genere,
	sono comuni partizionamenti in 5 (5-fold cross-validation)
	o in 10 (10-fold cross validation) sotto-dataset.

	Nel caso in cui la dimensione del dataset a disposizione
	sia estremamente limitata, é possibile adottare un approccio
	radicale chiamato \strong{leave-one-out cross-validation}, o
	\strong{LOOCV}. L'approccio é di fatto un n-fold cross-validation
	dove la dimensione del test set é unitaria, ovvero se il dataset
	é composto da \math{n} elementi, il sotto-dataset utilizzato per
	la costruzione del modello ha dimensione \math{n - 1} mentre
	quello utilizzato per il testing ha dimensione 1. Come per il
	caso precedente, si ripete l'operazione \math{n} volte e si
	ricava l'accuratezza complessiva a partire dalla media delle
	\math{n} accuratezze parziali.

	Talvolta, trasversalmente a questi approcci, viene introdotto
	un terzo sotto-dataset, chiamato \strong{validation set}.
	Questo viene utilizzato per valutare le prestazioni degli
	\strong{iperparametri}, ovvero i parametri che determinano
	la scelta stessa del tipo di modello. Il modello i cui
	iperparametri sono i piu performanti viene poi allenato con
	il training set e testato con il test set come di consueto.

\end{sile}
