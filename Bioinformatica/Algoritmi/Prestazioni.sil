\begin{sile}

	Esistendo diversi algoritmi per risolvere uno stesso problema, é utile poter 
	comparare fra loro algoritmi diversi per valutare quale tra questi sia il 
	"migliore". Nella maggior parte dei casi non esiste un criterio univoco per
	determinare quando un algoritmo sia migliore di un altro, sia perché alcuni
	aspetti prestazionali di un algoritmo possono essere piú o meno utili a 
	seconda di come lo si voglia implementare, sia perché uno stesso problema 
	é meglio risolto da un algoritmo piuttosto che da un altro in base a quali
	condizioni lo delimitano.

	Un parametro di valutazione molto importante per l'analisi algoritmica é 
	il \bf{tempo di esecuzione}, ovvero il numero di operazioni primitive che 
	l'algoritmo esegue per un input generico. Per semplificarne il calcolo, si 
	preferisce assumere che l'input di un algoritmo avente dimensione \math{n} 
	sia sempre proporzionale a \math{ln(n)}; questo non é, in genere, uno 
	scenario realistico, ma permette di considerare ogni operazione primitiva 
	di un algoritmo come non dipendente dalla dimensione dell'input. In termini 
	pratici, una singola riga di codice puó essere pensata come avente costo dove 
	\math{c_{i}} é una costante indipendente dall'input. Naturalmente, se una 
	primitiva dell'algoritmo avente costo \math{c_{i}} deve venire eseguita 
	\math{k} volte, allora il tempo di esecuzione relativo a quella istruzione 
	é \math{k \cdot c_{i}}. Il tempo di esecuzione complessivo é dato dalla somma 
	dei tempi di esecuzione parziali delle singole istruzioni.

	\begin{examplebox}
		Si consideri l'algoritmo di ordinamento Insertion sort. L'istruzione 1 
		viene necessariamente eseguita tante volte quanto vale la dimensione 
		dell'input, ovvero \math{n}, perché per ciascun valore dell'array in 
		input é necessario incrementare il contatore \math{i} di 1 e valutare 
		se questo sia inferiore alla lunghezza dell'array. Le istruzioni 2, 3 
		e 7 vengono eseguite tante volte quanto viene eseguita la prima istruzione 
		meno uno, ovvero \math{n - 1}, perché deve venire eseguita per ogni 
		iterazione del ciclo \tt{for} meno l'ultima, in cui viene fatto il 
		confronto e risulta violato. 

		Studiare l'istruzione 4 é piú complesso, perché il numero di volte in cui 
		viene eseguita non é determinabile a priori, perché dipende dall'input: se 
		l'array é parzialmente ordinato verrá eseguita poche volte, mentre se é poco 
		ordinato verrá eseguita molte volte. Sia allora \math{m_{i}} il numero di 
		volte (quale che sia) che l'istruzione numero 4 viene eseguita per il 
		\math{i}-esimo valore; il numero totale di volte in cui viene eseguita 
		l'istruzione 4 sará allora la somma di tutti i valori di \math{m_{i}}. 
		Il numero di volte in cui vengono eseguite le istruzioni 5 e 6, similmente
		a quanto é stato detto per le istruzioni 2, 3 e 7, equivale al numero di 
		volte in cui é stata eseguita 4 meno uno.

		\par
		\begin[width = 60%fw]{parbox}
			\begin{verbatim}
				1) for i in range (0, len(values)):
				2)     temp = values[i]
				3)     j = i - 1
				4)     while (j >= 0 and values[j] > temp):
				5)         values[j + 1] = values[j]
				6)         j = j - 1
				7)     values[j + 1] = temp
			\end{verbatim}
		\end{parbox}
		\begin[width = 40%fw]{parbox}
			\begin[cols = 7.5%fw 42.5%fw 7.5%fw 42.5%fw, cellborder = 0]{ptable}
				\begin{row}
					\begin{cell}
						\math{1)}
					\end{cell}
					\begin{cell}
						\math{c_{1} \cdot n}
					\end{cell}
					\begin{cell}
						\math{5)}
					\end{cell}
					\begin{cell}
						\math{c_{5} \cdot \sum_{i = 0}^{n - 1} (m_{i} - 1)}
					\end{cell}
				\end{row}
				\begin{row}
					\begin{cell}
						\math{2)}
					\end{cell}
					\begin{cell}
						\math{c_{2} \cdot (n - 1)}
					\end{cell}
					\begin{cell}
						\math{6)}
					\end{cell}
					\begin{cell}
						\math{c_{6} \cdot \sum_{i = 0}^{n - 1} (m_{i} - 1)}
					\end{cell}
				\end{row}
				\begin{row}
					\begin{cell}
						\math{3)}
					\end{cell}
					\begin{cell}
						\math{c_{3} \cdot (n - 1)}
					\end{cell}
					\begin{cell}
						\math{7)}
					\end{cell}
					\begin{cell}
						\math{c_{7} \cdot (n - 1)}
					\end{cell}
				\end{row}
				\begin{row}
					\begin{cell}
						\math{4)}
					\end{cell}
					\begin{cell}
						\math{c_{4} \cdot \sum_{i = 0}^{n - 1} m_{i}}
					\end{cell}
					\cell{}
					\cell{}
				\end{row}
			\end{ptable}
		\end{parbox}
		\par\bigskip

		\begin[mode = display]{math}
			T{(n)} = c_{1} \cdot n + 
				   c_{2} \cdot {(n - 1)} + 
				   c_{3} \cdot {(n - 1)} + 
				   c_{4} \cdot \sum_{i = 0}^{n - 1} m_{i} + 
				   c_{5} \cdot \sum_{i = 0}^{n - 1} {(m_{i} - 1)} + 
				   c_{6} \cdot \sum_{i = 0}^{n - 1} {(m_{i} - 1)} + 
				   c_{7} \cdot {(n - 1)}
		\end{math}
	\end{examplebox}

	Naturalmente, l'input passato ad un algoritmo influisce su quante volte deve 
	eseguire i suoi passi. Ad esempio, se ad un algoritmo di ordinamento viene
	passato come input un array giá parzialmente ordinato sará necessario eseguire 
	meno istruzioni rispetto al passarvi un array con i valori disposti in maniera
	completamente casuale. Per questo motivo si preferisce non valutare il tempo
	di esecuzione su un input generico, ma piuttosto in due casi: il \bf{caso 
	migliore} (\em{best case}) ed il caso peggiore (\em{worst case}). Il primo é
	il caso che si verifica quando all'algoritmo viene passato un input per il 
	quale deve eseguire le sue istruzioni il minimo numero di volte, mentre il 
	secondo é il caso che si verifica quando all'algoritmo viene passato un input 
	per il quale deve eseguire le sue istruzioni il massimo numero di volte. 
	Individuare i due casi non é una operazione scontata, ed algoritmi diversi
	hanno casi migliori/peggiori diversi.

	\begin{examplebox}
		Per quanto riguarda Insertion sort, il caso migliore si ha quando i 
		valori passati in input sono giá di per loro ordinati, perché l'istruzione 
		4 viene eseguita una volta sola per ciascun ciclo e le istruzioni 5 e 6 
		(che dipendono dalla 4) non vengono mai eseguite. Pertanto, nel caso 
		migliore si ha che \math{m_{i}} vale sempre 1 per ogni valore di \math{i}.

		\begin[mode = display]{math}
			\table[columnalign = left left]{
				T_{best}(n) & = c_{1} \cdot n + 
						    c_{2} \cdot {(n - 1)} + 
						    c_{3} \cdot {(n - 1)} + 
						    c_{4} \cdot \sum_{i = 0}^{n - 1} 1 + 
						    c_{5} \cdot \sum_{i = 0}^{n - 1} {(1 - 1)} + 
						    c_{6} \cdot \sum_{i = 0}^{n - 1} {(1 - 1)} + 
						    c_{7} \cdot {(n - 1)} = \\
						& = c_{1} \cdot n + 
						    c_{2} \cdot {(n - 1)} + 
						    c_{3} \cdot {(n - 1)} + 
						    c_{4} \cdot {(n - 1)} + 
						    c_{7} \cdot {(n - 1)} = \\
						& = c_{1} \cdot n + 
						    c_{2} \cdot n - c_{2} +
						    c_{3} \cdot n - c_{3} +
						    c_{4} \cdot n - c_{4} +
						    c_{7} \cdot n - c_{7} = \\
						& = (c_{1} + c_{2} + c_{3} + c_{4} + c_{7}) n -
						    (c_{2} + c_{3} + c_{4} + c_{7}) \\
			}
		\end{math}

		Il caso peggiore si ha invece quando i valori passati in input sono ordinati
		in ordine decrescente, perché l'istruzione 4 deve venire eseguita tante volte 
		quanto vale \math{i}, in quanto tutti i valori devono venire fatti scalare in 
		prima posizione. É intuibile come, di conseguenza, le istruzioni 5 e 6 debbano 
		venire eseguite \math{i - 1} volte.

		\begin[mode = display]{math}
			\table[columnalign = left right]{
				\sum_{i = 0}^{n - 1} i = 
				{(\frac{(n - 1)(n - 1 + 1)}{2})} = 
				\frac{n(n - 1)}{2} &
				\sum_{i = 0}^{n - 1} {(i - 1)} = 
				{(\frac{(n - 1)(n - 1 + 1)}{2} - {(n - 1)})} = 
				\frac{(n - 2)(n - 1)}{2} \\
			}
		\end{math}

		\begin[mode = display]{math}
			\table[columnalign = left left]{
				T_{worst}(n) & = c_{1} \cdot n + 
						     c_{2} \cdot {(n - 1)} + 
						     c_{3} \cdot {(n - 1)} + 
						     c_{4} \cdot \sum_{i = 0}^{n - 1} i + 
						     c_{5} \cdot \sum_{i = 0}^{n - 1} {(i - 1)} + 
						     c_{6} \cdot \sum_{i = 0}^{n - 1} {(i - 1)} + 
						     c_{7} \cdot {(n - 1)} = \\
						 & = c_{1} \cdot n + 
						     c_{2} \cdot n - c_{2} + 
						     c_{3} \cdot n - c_{3} + 
						     \frac{c_{4}}{2} \cdot n{(n - 1)} + 
						     \frac{c_{5}}{2} \cdot {(n - 1)}{(n - 2)} + 
						     \frac{c_{6}}{2} \cdot {(n - 1)}{(n - 2)} + 
						     c_{7} \cdot n - c_{7} = \\
						 & = {(\frac{1}{2} c_{4} + \frac{1}{2} c_{5} + \frac{1}{2} c_{6})}n^{2} + 
						     {(c_{1} + c_{2} + c_{3} - \frac{1}{2} c_{4} - \frac{3}{2} c_{5} - \frac{3}{2} c_{6} + c_{7})}n - 
						     {(c_{2} + c_{3} - c_{5} - c_{6} + c_{7})} \\
			}
		\end{math}

		Si noti come le istruzioni 1, 2, 3 e 7 dipendano esclusivamente dalla 
		dimensione dell'input, pertanto vengono eseguite comunque a prescindere 
		che si stia trattando il caso migliore o peggiore.
	\end{examplebox}

	Analizzare il tempo di esecuzione di algoritmi ricorsivi é molto piú 
	complesso. Algoritmi ricorsivi possono essere analizzati mediante una
	\bf{relazione di ricorrenza}: il tempo richiesto da una routine é uguale
	al tempo speso all'interno della routine piú il tempo speso per le chiamate
	ricorsive. Questa relazione di ricorrenza é una equazione dove la variabile
	tempo compare sia a sinistra che a destra.

	Un possibile approccio é il cosiddetto \bf{metodo dell'iterazione}, che 
	prevede di "sciogliere" la ricorsione per esprimerla sotto forma di una 
	sommatoria dipendente solo da \math{n}. Applicare questo metodo non é 
	sempre facile, perché richiede di utilizzare manipolazioni algebriche 
	non scontate.

	\begin{examplebox}
		\begin[width = 25%fw]{parbox}
			\begin[mode = display]{math}
				T(n) = \{\table[columnalign = left right]{
					c + T(\frac{n}{2}) & \mo[atom = bin]{se} n > 1 \\
					1 & \mo[atom = bin]{se} n = 1\\
				}
			\end{math}
		\end{parbox}
		\begin[width = 75%fw]{parbox}
			\begin[mode = display]{math}
				T(n) = 
				c + T(\frac{n}{2}) = 
				2c + T(\frac{n}{4}) = 
				3c + T(\frac{n}{8}) = 
				\unicodeellipsis = 
				i \cdot c + T(\frac{n}{2^{i}})
			\end{math}
		\end{parbox}
		\par

		Il procedimento va iterato finché non si raggiunge il passo base. Questo 
		accade quando \math{n/2^{i} = 1}, ovvero quando \math{i = log_{2}(n)}. 
		Ponendo \math{i = log_{2}(n)} é possibile concludere \math{T(n) = c log(n) 
		+ T(1) \in O(log(n))}.
	\end{examplebox}

	Se la relazione di ricorrenza é forma presentata di seguito, é possibile 
	ricavare il tempo di esecuzione mediante un teorema chiamato \bf{teorema 
	fondamentale delle ricorrenze}:

	\par
	\begin[width = 25%fw]{parbox}
		\begin[mode = display]{math}
			T(n) = \{\table[columnalign = left right]{
				aT(\frac{n}{b}) + f(n) & \mo[atom = bin]{se} n > 1 \\
				1 & \mo[atom = bin]{se} n = 1\\
			}
		\end{math}
	\end{parbox}
	\begin[width = 75%fw]{parbox}
		\begin{enumerate}
			\begin{item}
				\math{T(n) = \Theta(n^{log_{b}(a)})} 
				se \math{f(n) = O(n^{log_{b}(a) - \epsilon})} 
				per \math{\epsilon > 0};
			\end{item}
			\begin{item}
				\math{T(n) = \Theta(n^{log_{b}a} log(n))} 
				se \math{f(n) = \Theta(n^{log_{b}(a)})};
			\end{item}
			\begin{item}
				\math{T(n) = \Theta(f(n))}, 
				se \math{f(n) = \Omega(n^{log_{b}(a + \epsilon)})} 
				per \math{\epsilon > 0} e \math{a \cdot f(n/b) \leq c \cdot f(n)} 
				per \math{c > 1} e \math{n} sufficientemente grande.
			\end{item}
		\end{enumerate}
	\end{parbox}
	\par\bigskip

	L'espressione del tempo di esecuzione di un algoritmo contiene spesso 
	molte costanti che rendono la funzione difficile da leggere ed inutilmente 
	complicata. Per questo motivo spesso non si é particolarmente interessati 
	a considerare il tempo di esecuzione di per sé, ma bensí una sua delimitazione
	che ne dia una stima ragionevole. A questo scopo é possibile introdurre la 
	\bf{notazione asintotica}, che permette di approssimare tempi di esecuzione 
	aventi una espressione complessa mediante espressioni piú semplici. Data una 
	funzione generica \math{f_{n}}:

	\begin{itemize}
		\begin{item}
			\begin[mode = display]{math}
				O(f(n)) = \{g(n) : \exists c > 0 \wedge n_{0} \geq 0 
				t.c. g(n) \leq cf(n) \forall n \geq n_{0}\}
			\end{math}
			In altri termini, se \math{g(n) \in O(f(n))} 
			allora \math{g(n)} cresce piú lentamente di \math{f(n)}
		\end{item}
		\begin{item}
			\begin[mode = display]{math}
				\Omega(f(n)) = \{g(n) : \exists c > 0 \wedge n_{0} \geq 0 
				t.c. g(n) \geq cf(n) \forall n \geq n_{0}\}
			\end{math}
			In altri termini, se \math{g(n) \in \Omega(f(n))} 
			allora \math{g(n)} cresce piú velocemente di \math{f(n)}
		\end{item}
		\begin{item}
			\begin[mode = display]{math}
				\Theta(f(n)) = \{g(n) : \exists c_{1}, c_{2} > 0 \wedge n_{0} \geq 0 
				t.c. c_{1}f(n) \leq g(n) \leq c_{2}f(n) \forall n \geq n_{0}\}
			\end{math}
			In altri termini, se \math{g(n) \in \Omega(f(n))} 
			allora \math{g(n)} cresce alla stessa velocitá di \math{f(n)}
		\end{item}
	\end{itemize}

	\begin{examplebox}
		É possibile verificare che, per Insertion Sort, 
		\math{T_{best} \in O(n)} e \math{T_{worst} \in O(n^{2})}.
		Ovvero, \math{T_{best}} e \math{T_{worst}} crescono piú
		rapidamente di, rispettivamente, \math{n} e \math{n^{2}}
	\end{examplebox}

\end{sile}
