#import "../AIModels_definitions.typ": *

When manipulating and comparing graphs, there is often interest in knowing
whether two graphs are *similar*. The notion of graph similarity is muddy,
since there is no set definition of what it means for two graphs to be
similar.

In general, it is impractical to directly compute the similarity (however
defined) between two graphs, because real graphs tend to have a very high
amount of nodes. A better approach consists in retrieving a reasonable
approximation of similarity by extracting features from the graph nodes.

The goal of these methods is to _encode_ nodes as low-dimensional vectors
that summarize their graph position and the structure of their local graph
neighborhood. The idea is to *embed* nodes into a *latent space* where
geometric relations between its elements correspond to relationships (that
is, edges) in the original graph.

This model requires a pair of operation, an *encoder* and a *decoder*.
An encoder is a function that maps each node in the graph into a
low-dimensional vector, whereas a decoder is a function that takes
the low-dimensional node embeddings and uses them to reconstruct
information about each node's neighborhood in the original graph.

For this mapping to work it is necessary to assign a unique identifier
to each node, so that the mapping is one-to-one. This can be done, given
an arbitrary order of the nodes, by assigning an *indicator vector* to
each node. The indicator vector $bold(v)_(i)$ for a node $v in V$ is
a binary vector that has $0$ in each component except a single component
which is $1$. In this way, each node has unique and unambiguous ID to be
referred to.

Formally, the encoder is a function $"Enc": V -> RR^(d)$ that maps nodes
$v in V$ to their respective vector embeddings $bold(z)_(v) in RR^(d)$,
where $d$ is the dimension of the latent space #footnote[Realistic
applications of encoders have latent spaces whose dimension ranges
from roughly $16$ to $1000$.]. The easiest way to define this function
is to adopt the *shallow embedding* approach, where the encoding function
simply performs a "lookup" on a matrix that contains all embedding vectors.
More formally:

#grid(
	columns: (0.5fr, 0.5fr),
	[$ "Enc"(v) = bold(Z)[v] = bold(Z) bold(v)_(i) = bold(Z)_(bold(v)_(i)) $],
	[$ bold(Z) = mat(mat(bold(Z)_(bold(v)_(1))),
	                 mat(bold(Z)_(bold(v)_(2))),
	                 mat(dots),
	                 mat(bold(Z)_(bold(v)_(abs(v))))) $]
)

Where $bold(Z) in RR^(abs(V) times d)$ is a matrix containing the
embedding vectors for all nodes, $bold(Z)[v]$ denotes the row of
$bold(Z)$ corresponding to node $v$ and $bold(v)_(i)$ is the
indicator vector for $v$. Note how shallow embedding is simply
a matrix multiplication.

The decoder's purpose is to reconstruct certain graph statistics from the
node embeddings that are generated by the encoder. For example, given a
node embedding $bold(z)_(v)$ of a node $v$, the decoder might attempt to
predict the neighborhood of $v$ or its row $A[v]$ in the graph adjacency
matrix.

The most common way to define a decoder is to define a *pairwise decoder*,
a function $"Dec": RR^(d) times RR^(d) -> RR^(+)$ that has two encodings
as input and a (predicted) non-zero value as output:

$ "Dec"("Enc"(u), "Enc"(v)) = "Dec"(bold(Z)[u], bold(Z)[v]) $

Many modern graph embedding approaches such as `GraRep` and `HOPE` define
decoding as the dot product between the encodings:

$ "Dec"(bold(Z)[u], bold(Z)[v]) = bold(Z)[u]^(T) bold(Z)[v] $

Of course, it is not possible to simply define the decoder as the inverse
of the encoder, because encoding entails a partial loss of information.
Therefore, the idea is to define the decoder so that the difference between
the "real" value of a similarity measure and its estimated value is as close
as possible:

$ "Dec"(bold(Z)[u], bold(Z)[v]) approx bold(S)[u, v] $

Where $bold(S)[u, v]$ is a generic graph-based similarity measure between the
nodes $u$ and $v$ and $bold(S) in RR^(abs(V) times abs(V))$ is the similarity
matrix summarizing all pairwise node similarities.

To achieve this "closeness" to the real similarity measure, the standard
practice is to minimize an empirical reconstruction loss $L$ over a set of
training node pairs $D$:

$ L = sum_((u, v) in D) l("Dec"(bold(Z)[u], bold(Z)[v]), bold(S)[u, v]) $

Where $l$ is any loss function measuring the discrepancy between the
decoded (that is, estimated) similarity values $"Dec"(bold(Z)[u],
bold(Z)[v])$ and the true values $bold(S)[u, v]$.

The overall objective is to train the encoder and the decoder so that
pairwise node relationships can be effectively reconstructed on the
training set $D$. The choice of $l$ depends on the definition of the
decoder and the similarity function, and different graph embedding
algorithms use different loss functions.

Algorithms such as `GraRep` and `HOPE` use _mean square error_ as loss
function, which would give:

$ L = sum_((u, v) in D) abs(abs("Dec"(bold(Z)[u], bold(Z)[v]) - bold(S)[u, v]))_(2)^(2) $

Other algorithms, such as `DeepWalk`, use cross entropy:

$ L = sum_((u, v) in D) -bold(S)[u, v] log("Dec"(bold(Z)[u], bold(Z)[v])) $

It is therefore necessary to give a definition of $bold(S)[u, v]$. As
stated before, there is no set definition of similarity, but many were
proposed. The easiest one is given by counting the number of neighbors
that are shared between the two nodes:

$ bold(S)[u, v] := abs(N(u) inter N(v)) $

A more refined approach is given by considering general neighborhood
overlap measures. One such example is the *Katz index*, obtained by
counting the number of paths of any length between a pair of nodes
and summing them together:

$ bold(S)[u, v] := S_("Katz")[u, v] = sum^(infinity)_(i = 1) beta^(i) (bold(A) [u, v])^(i) $

Where $beta$ is a user-defined parameter that tunes how much weight is given
to short versus long paths. A small value of $beta$ would give short paths
more weights and long paths less weight, for example. The value of $beta$
must be strictly positive and must be less than the greatest eigenvalue of
$bold(A)$.

General neighborhood overlap measures are employed as the notion of
similarity by graph embedding algorithms such as `HOPE`, whereas
algorithms such as `GraRep` define $bold(S)$ based on powers of the
adjacency matrix.

Note that, even though easy to perform, shallow encoding suffers from some
problems. Notably:

- Does not share parameters between nodes on the encoding;
- Does not preserve node features, only indexing;
- Is *transductive*: applies only to the nodes present during the
  training phase. 
