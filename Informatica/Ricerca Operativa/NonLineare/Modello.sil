\begin{sile}

    Sebbene l'ipotesi di linearitá permetta di semplificare notevolmente il
    modello di programmazione lineare, in diverse classi di problemi questa
    assunzione rende il modello troppo semplicistico. In questi casi, occorre
    rinunciare all'ipotesi di linearitá e ammettere che uno o piú elementi
    del modello (la funzione obiettivo, i vincoli o entrambi) possano essere
    funzioni non lineari. Questo significa che, in modelli di questo tipo,
    nella funzione obiettivo e/o nei vincoli possono comparire variabili di
    grado superiore al primo.

    Problemi in cui la funzione obiettivo e/o uno o piú vincoli sono
    funzioni non lineari prendono il nome di \strong{problemi di
    programmazione nonlineare} (\strong{PNL}). La forma standard di 
    problemi di programmazione nonlineare é essenzialmente la medesima
    dei problemi di programmazione lineare:

    \smallskip
    \begin{center}
        Determinare
        \math{\bi{x}* = (x_{1}, x_{2}, \unicodeellipsis, x_{n})}
        tale da massimizzare
        \math{f(\bi{x})}
        soggetto ai vincoli
        \begin{math}
            \{\table[columnalign = left]{
                g_{i}(\bi{x}) \leq b_{i} \thickspace i = 1, 2, \unicodeellipsis, m \\
                \bi{x} \geq \bi{0} \\
            }
        \end{math}
    \end{center}
    \bigskip

    Dove \math{f(\bi{x})} e \math{g_{i}(\bi{x})} sono funzioni note nelle
    \math{n} variabili decisionali.

    La programmazione nonlineare presenta diverse sfide in piú rispetto
    alla programmazione lineare. Innanzitutto, il metodo del simplesso
    risulta inapplicabile perché la natura della regione ammissibile
    non é la medesima della programmazione lineare.

    Se un problema di programmazione matematica presenta un vincolo
    non lineare, la regione ammissibile non é piú un politopo convesso,
    perché uno o piú lati di quest'ultima sono composti da coniche.
    Questo significa non solo che la soluzione ottimale potrebbe non
    trovarsi su uno dei vertici della regione ammissibile, ma che
    quest'ultima potrebbe non avere alcun vertice.

    D'altro canto, se un problema di programmazione matematica
    presenta una funzione obiettivo non lineare, il valore ottimale
    della funzione obiettivo non é piú dato dall'intersezione di un
    piano \math{n}-dimensionale con la regione ammissibile ma
    dall'intersezione di una conica \math{n}-dimensionale. Questo
    significa non solo che la soluzione ottimale potrebbe non trovarsi
    su uno dei vertici della regione ammissibile, ma che la soluzione
    ottimale potrebbe non trovarsi nemmeno sulla frontiera di quest'ultima.

    Piú in generale, il problema sussiste perché i vincoli funzionali di un
    problema di programmazione nonlineare non sono necessariamente funzioni
    convesse. Dato che la regione ammissibile di un problema di programmazione
    matematica é data dall'intersezione dei vincoli, se tali vincoli non sono
    funzioni convesse allora la regione ammissibile non sará un insieme
    convesso. Se questo accade, un massimo locale nella regione ammissibile
    non é necessariamente anche un massimo globale, e gli algoritmi di
    programmazione nonlineare non sono in grado di fare distinzione tra i
    due \footnote{La distinzione fra massimi locali e massimi globali non si
    presentava nel caso della programmazione lineare. Questo avviene perché,
    come dimostrato in precedenza, le funzioni lineari sono sempre convesse
    (e anche sempre concave), pertanto la regione ammissibile di un problema
    di programmazione lineare é sempre un insieme convesso.}.

    Vi sono diverse classi di problemi di programmazione non
    lineare, a seconda delle caratteristiche di \math{f(\bi{x})}
    e \math{g_{i}(\bi{x})}. Per ciascuna di queste, esistono
    algoritmi ad hoc in grado di risolverli: questi fanno uso
    di una strategia di tipo \strong{line-search}, ovvero
    generano una successione di punti all'interno dello spazio
    \math{\dsi{R}^{n}} in modo che il punto successivo é ottenuto
    a partire dal precedente muovendosi lungo una \strong{direzione
    di salita} o \strong{direzione di discesa}.

    Data una funzione \math{f : X \subseteq \dsi{R}^{n} \mapsto
    \dsi{R}} ed un punto \math{\bi{x} \in \dsi{R}^{n}} in cui
    \math{f} é differenziabile, una direzione di discesa é definita
    come un generico vettore \math{\bi{v} \in \dsi{R}^{n}} tale per
    cui il prodotto scalare fra quest'ultimo ed il gradiente di 
    \math{f} nel punto \math{\bi{x}} é strettamente negativo. Allo
    stesso modo, una direzione di salita é definita come un generico
    vettore \math{\bi{v} \in \dsi{R}^{n}} tale per cui il prodotto
    scalare fra quest'ultimo ed il gradiente di \math{f} nel punto
    \math{\bi{x}} é strettamente positivo.

    Data una funzione \math{f : X \subseteq \dsi{R}^{n} \mapsto \dsi{R}} ed
    un punto \math{\bi{x} \in \dsi{R}^{n}} in cui \math{f} é differenziabile,
    gli algoritmi che applicano la strategia line-search possono essere tutti
    ridotti a questo insieme di passaggi:

    \begin{enumerate}
        \begin{item}
            Un indice \math{k}, che indica l'iterazione corrente,
            viene inizializzato a 0. Il punto \math{\bi{x}^{k}}
            indica l'approssimazione finora trovata per il massimo 
            o il minimo di \math{f};
        \end{item}
        \begin{item}
            Viene calcolata la direzione di discesa \math{\bi{d}^{k}} sulla
            base di \math{\bi{x}^{k}};
        \end{item}
        \begin{item}
            Viene determinato il punto successivo \math{\bi{x}^{k + 1}} lungo
            la direzione \math{\bi{x}^{k}} come \math{\bi{x}^{k} \pm \alpha^{k}
            \bi{d}^{k}}, dove \math{\alpha^{k}} é una quantitá scalare positiva
            chiamata \strong{step-size};
        \end{item}
        \begin{item}
            Se \math{\bi{x}^{k + 1}} é una approssimazione accettabile per il
            massimo o il minimo di \math{f} l'algoritmo termina, altrimenti 
            si riprende dal punto 2 con \math{\bi{x}^{k + 1}} come nuovo punto
            in considerazione.
        \end{item}
    \end{enumerate}

    \bigskip

    Di fatto, la differenza fra gli algoritmi di line-search sta nel modo in
    cui questi calcolano lo step-size.

\end{sile}
