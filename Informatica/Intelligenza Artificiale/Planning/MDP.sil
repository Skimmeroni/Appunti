\begin{sile}

	Viene chiamato \strong{Markov Decision Process} (\strong{MDP})
	un problema di ricerca in cui vale l'assunzione Markoviana e
	dove l'ambiente é accessibile ma non deterministico. Ovvero,
	l'agente sa sempre in che stato si trova ma non ha la certezza
	che compiere una azione porterá allo stato che si aspetta. Un
	MDP é costituito da:

	\begin{itemize}
		\begin{item}
			Un insieme discreto di stati \math{S};
		\end{item}
		\begin{item}
			Un insieme discreto di azioni \math{A};
		\end{item}
		\begin{item}
			Uno stato iniziale \math{s_{0} \in S};
		\end{item}			
		\begin{item}
			Un modello di transizione \math{T(s, a, s')}, con
			\math{a \in A} e \math{s, s' \in S}. Questo indica
			qual'é la probabilitá che venga effettivamente
			raggiunto lo stato \math{s'} eseguendo \math{a}
			mentre ci si trova in \math{s}. In termini di calcolo
			delle probabilitá, essendo valida l'assunzione
			Markoviana, \math{T(s, a, s')} equivale a
			\math{P(s' | s, a)}.
		\end{item}
		\begin{item}
			Una \strong{funzione di ricompensa} \math{R(s, a, s')},
			che associa un valore numerico a ciascuna transizione.
			Tale valore rappresenta quanto é "vantaggioso" per
			l'agente compiere la transizione da \math{s} a
			\math{s'} mediante \math{a};
		\end{item}
		\begin{item}
			Opzionalmente, uno stato finale \math{s_{f} \in S}.
		\end{item}
	\end{itemize}

	\bigskip

	A differenza dei problemi di ricerca, dove si cerca di
	minimizzare la funzione di costo, negli MDP si cerca di
	massimizzare la funzione di ricompensa. Infatti, negli
	MDP lo stato finale non e sempre presente, perche l'obiettivo
	dell'agente potrebbe semplicemente consistere nel raggiungere
	un certo valore per \math{R(s, a, s')}, a prescindere da
	quale sia lo stato in cui si trova quando questo accade.

	La differenza piu rilevante fra i due si ha pero nella forma
	della loro soluzione. Nei problemi di ricerca la soluzione é
	una sequenza di azioni che conducono dallo stato iniziale
	ad uno stato obiettivo; negli MDP, questo approccio non e
	possibile, perché una certa sequenza di azioni é in grado
	di portare da uno stato ad un altro solamente con una certa
	probabilitá. Questo significa che eseguendo piu volte la
	stessa sequenza di azioni potrebbe venire raggiunto uno
	stato completamente diverso da esecuzione ad esecuzione.

	L'azione da eseguire in un certo stato puó essere pensata
	come una variabile aleatoria, alla quale é associata una
	probabilitá per ciascun valore che questa puó assumere.
	Pertanto, una soluzione per agenti probabilistici deve
	specificare cosa un agente debba fare in \em{ogni} stato
	in cui l'agente potrebbe trovarsi. A tale scopo, é necessario
	introdurre il concetto di \strong{politica}, la soluzione di
	un MDP.

	Una politica \math{\pi} e una funzione che mappa ciascuno
	stato del problema ad un'azione \footnote{Il motivo per
	cui e stato specificato che gli MDP hanno un numero finito
	di stati e che altrimenti non sarebbe possibile definire una
	politica.}; dato uno stato \math{s}, \math{\pi(s)} é l'azione
	che la politica \math{\pi} "raccomanda" di eseguire se ci si
	trova in \math{s}. La politica migliore \math{\pi *}, detta
	\strong{politica ottimale}, e quella che in ogni stato
	raccomanda l'azione che restituisce il massimo valore di
	ricompensa possibile.

	Ogni volta che una determinata politica viene eseguita
	a partire dallo stato iniziale, la natura stocastica
	dell'ambiente porta a generare diverse sequenze di azioni,
	ciascuna con una propria probabilità. La "qualità" di una
	politica viene pertanto misurata a partire dall'utilità
	\em{attesa} delle possibili sequenze di azioni generate
	da tali politiche. Pertanto, la politica ottimale e
	quella che massimizza tale valore di utilita attesa.

	%% Esempio preso dal libro. Sulla base di come costruisco \math{R(s)}
	%% cambia la politica ottimale. Se \math{R(s) \leq -1.6284} per tutti
	%% gli stati tranne quelli terminali, la politica migliore prevede che
	%% l'agente si diriga allo stato \math{-1} perchè è comunque preferibile
	%% a raggiungere lo stato con \math{+1}. Se \math{-0.4278 < -0.0850} la
	%% politica migliore prevede che l'agente si accolli il rischio perchè
	%% la penalità è comunque troppo alta. Se \math{-0.0221 < R(s) \leq 0}
	%% la politica migliore prevede che l'agente non si accolli il rischio
	%% perchè la penalità è bassa e non ne vale la pena. Se \math{R(s) > 0},
	%% la politica migliore prevede che l'agente continui a muoversi a caso
	%% senza mai andare negli stati terminali perchè così continua a guadagnare
	%% ricompensa all'infinito. CORREDARE IL TUTTO CON I DISEGNINI.

	Data la sequenza ordinata \math{S} di \math{n} stati
	\math{[s_{0}, s_{1}, \unicodeellipsis, s_{n}]}, sia
	\math{U(S)} la ricompensa complessiva ricavata
	dall'attraversare ordinatamente tale sequenza di stati.
	Esistono diverse regole per calcolare la ricompensa
	complessiva; la piu semplice è la \strong{ricompensa
	additiva}, dove la ricompensa totale è semplicemente
	la somma delle ricompense associate ai singoli stati:

	\begin[mode = display]{math}
		U{(S)} = U{({[s_{0}, s_{1}, \unicodeellipsis, s_{n}]})} =
		R{(s_{0}, \pi(s_{0}), s_{1})} + R{(s_{1}, \pi(s_{1}), s_{2})} +
		\unicodeellipsis + R{(s_{n - 1}, \pi(s_{n - 1}), s_{n})}
		= \sum_{i = 0}^{n - 1} R{(s_{i}, \pi{(s_{i})}, s_{i + 1})}
	\end{math}

	Il problema di questa forma di assegnazione della ricompensa e
	che, in molte situazioni, incentiva l'agente a non raggiungere
	mai l'obiettivo, perche puo guadagnare ricompensa quasi
	indefinitamente. Una regola alternativa è la \strong{ricompensa
	con discount}, dove la ricompensa associata al trovarsi in un
	determinato stato decresce di una certa percentuale ad ogni
	cambio di stato.  Indicando con \math{\gamma} un valore
	compreso fra 0 e 1, la ricompensa totale adoperando tale
	metodo è data da:

	\begin[mode = display]{math}
		U{(S)} = U{({[s_{0}, s_{1}, \unicodeellipsis, s_{n}]})} =
		R{(s_{0}, \pi{(s_{0})}, s_{1})} + \gamma R{(s_{1}, \pi{(s_{1})}, s_{2})} +
		\unicodeellipsis + \gamma^{n - 1} R{(s_{n - 1}, \pi{(s_{n - 1})}, s_{n})}
		= \sum_{i = 0}^{n - 1} \gamma^{i} R{(s_{i}, \pi{(s_{i})}, s_{i + 1})}
	\end{math}

	\math{\gamma} determina quanta priorità debba dare l'agente
	al raggiungere determinati stati in una determinata iterazione.
	Se \math{\gamma} è un valore prossimo a 0, le ricompense date
	dagli stati raggiunti nelle prime iterazioni hanno un peso
	molto maggiore sul valore di ricompensa complessivo rispetto a
	quelle fornite dagli stati raggiunti nelle ultime iterazioni.
	Se \math{\gamma} è un valore prossimo ad 1, le ricompense date
	dagli stati raggiunti nelle prime iterazioni e nelle ultime
	iterazioni hanno un peso comparabile. Se \math{\gamma} è
	esattamente 1, non vi è alcuna differenza nel raggiungere uno
	stato in una certa iterazione piuttosto che in un'altra, e la
	regola con discount coincide di fatto con la regola additiva.

	\begin{theorem}
		Usando la regola con discount, la ricompensa complessiva
		\math{U} è un valore limitato, e pertanto non puo crescere
		indefinitamente.

		\bigskip
		\strong{Dimostrazione}. Sia data la sequenza \math{S}
		di \math{n} stati \math{[s_{0}, s_{1}, \unicodeellipsis,
		s_{n}]} che l'agente attraversa in un problema MDP. Essendo
		per questo \math{S} finita, deve esserlo anche la sequenza
		di ricompense \math{[R(s_{0}, \pi(s_{0}), s_{1}), R(s_{1},
		\pi(s_{1}), s_{2}), R(s_{n - 1}, \pi(s_{n - 1}), s_{n})]}.
		Ne consegue che esiste un elemento di tale sequenza, sia
		questo \math{R_{max}}, che e maggiore o uguale a tutte le
		altre ricompense.

		Si noti come \math{1 + \gamma + \gamma^{2} +
		\unicodeellipsis + \gamma^{n}} sia una serie
		geometrica; se \math{\gamma} è un valore compreso
		fra 0 e 1, vale:

		\begin[mode = display]{math}
			\mi{lim}_{n \rightarrow +\infty} 1 + \gamma + \gamma^{2} +
			\unicodeellipsis + \gamma^{n} = \mi{lim}_{n \rightarrow
			+\infty} \sum_{i = 0}^{n} \gamma^{i} = \frac{1}{1 - \gamma}
			\thickspace\Rightarrow\thickspace
			1 + \gamma + \gamma^{2} + \unicodeellipsis + \gamma^{n - 1}
			\leq \frac{1}{1 - \gamma}
		\end{math}

		Moltiplicando ambo i membri per \math{R_{max}}:

		\begin[mode = display]{math}
			R_{max} \sum_{i = 0}^{n - 1} \gamma^{i} \leq
			R_{max} {(\frac{1}{1 - \gamma})}
			\thickspace\Rightarrow\thickspace
			R_{max} + R_{max}\gamma + R_{max}\gamma^{2} +
			\unicodeellipsis + R_{max}\gamma^{n - 1}
			\leq \frac{R_{max}}{1 - \gamma}
		\end{math}

		Essendo \math{R_{max}} maggiore di tutti i valori
		in \math{[R(s_{0}, \pi(s_{0}), s_{1}), \unicodeellipsis,
		R(s_{n - 1}, \pi(s_{n - 1}), s_{n})]}, é possibile
		effettuare la seguente minorazione:

		\begin[mode = display]{math}
			R{(s_{0}, \pi(s_{0}), s_{1})} + \gamma R{(s_{1},
			\pi(s_{1}), s_{2})} + \gamma^{2} R{(s_{2}, \pi(s_{2}),
			s_{3})} + \unicodeellipsis + \gamma^{n - 1} R{(s_{n - 1},
			\pi(s_{n - 1}), s_{n})} \leq \frac{R_{max}}{1 - \gamma} 
		\end{math}

		Ovvero, \math{U \leq R_{max} / (1 - \gamma)}. Essendo
		\math{0 < \gamma < 1} ed essendo sia \math{R_{max}}
		che \math{U} valori positivi, \math{U} e effettivamente 
		limitato, e non puo crescere oltre tale limite.
	\end{theorem}

	Si assuma di utilizzare la regola con discount. Ci
	si chiede come ricavare, dato un MDP, la politica
	migliore. Innanzitutto, per ricavare la politica
	migliore non e possibile semplicemente massimizzare
	la funzione \math{U}, perche questa opera su una
	sequenza di stati e, come gia detto, una sequenza
	di stati non e una soluzione accettabile per un MDP.
	La politica migliore e invece quella che massimizza
	la ricompensa totale \em{attesa}.

	Sia \math{r_{t}} la ricompensa ottenuta dall'agente
	in un MDP al tempo \math{t} (a prescindere da quale
	sia lo stato raggiunto che la ha indotta). \math{r_{t}}
	puo essere pensato come una variabile aleatoria che
	assume un valore diverso a seconda di ciascuna esecuzione
	dell'MDP. Sia \math{R_{t}} la somma parziale con discount
	di tutte le ricompense ottenute a partire da \math{r_{t}}:

	\begin[mode = display]{math}
		R_{t} = r_{t} + \gamma r_{t + 1} + \gamma^{2} r_{t + 2}
		+ \unicodeellipsis = \sum_{i = 0}^{+\infty} \gamma^{i}
		r_{t + i}
	\end{math}

	\math{R_{t}} e essa stessa una variabile aleatoria. Per
	indicarne il valore a cui tende per \math{i \rightarrow
	+\infty}, se ne calcola il valore atteso:

	\begin[mode = display]{math}
		\dsi{R}_{t} = E{[R_{t}]} = E{[\sum_{i = 0}^{+\infty}
		\gamma^{i} r_{t + i}]} = \sum_{i = 0}^{+\infty}
		E{[\gamma^{i} r_{t + i}]}
	\end{math}

	Per la linearita del valore atteso, e possibile riscrivere
	l'equazione in forma ricorsiva:

	\begin[mode = display]{math}
		\dsi{R}_{t} = \sum_{i = 0}^{+\infty}
		E{[\gamma^{i} r_{t + i}]} = 
		E{[r_{t}]} + \sum_{i = 1}^{+\infty}
		E{[\gamma^{i} r_{t + i}]} = E{[r_{t}]} +
		\gamma \dsi{R}_{t + 1}
	\end{math}

	Sia \math{s_{0}} lo stato in cui viene ottenuta la
	ricompensa \math{r_{t}}. La ricompensa complessiva
	ottenuta dall'agente partendo dallo stato \math{s_{0}}
	e seguendo le azioni suggerite dalla politica \math{\pi}
	viene indicata con \math{U^{\pi}(s_{0})}:

	\begin[mode = display]{math}
		U^{\pi} {(s_{0})} = E{[\sum_{t = 0}^{+\infty} \gamma^{t}
		R{(s_{t}, \pi {(s_{t})}, s_{t + 1})}]}
	\end{math}

	\math{U^{\pi}} puo essere calcolato per tutti gli stati possibili.

	Viene chiamata \strong{funzione di azione-utilità} o
	\strong{Q-function} la funzione così definita:

	\begin[mode = display]{math}
		Q^{\pi}(s, a) = \dsi{R}_{t} | (s_{t}, a_{t}, \pi) =
		E[r_{t} | s_{t}, a_{t}] + \gamma \dsi{R}_{t} | s_{t}, a_{t},
		\pi
	\end{math}

	Ovvero, la ricompensa cumulativa attesa nell'istante di tempo
	\math{t} se ci si trova nello stato \math{s_{t}} avendo compiuto
	l'azione \math{a_{t}} suggerita da \math{\pi}.

	Una proprietá interessante é che, se si sta considerando la
	politica ottimale, le equazioni per \math{U(s)} e per \math{Q(s, a)}
	non dipendono dallo stato iniziale, ma hanno la stessa forma per
	qualsiasi stato iniziale venga scelto. Diventa allora possibile
	scrivere le due equazioni in forma ricorsiva, equazioni che prendono
	il nome di \strong{Equazioni di Bellman}:

	\begin[width = 35%fw]{parbox}
		\begin[mode = display]{math}
			U{(s)} = \mi{max}_{a \in A(s)} \sum_{s'} P{(s' | s, a)} {[R{(s, a, s')} + \gamma U{(s')}]}
		\end{math}
	\end{parbox}
	\begin[width = 65%fw]{parbox}
		\begin[mode = display]{math}
			Q{(s, a)} = \sum_{s'} P{(s' | s, a)} {[R{(s, a, s')} + \gamma U{(s')}]} =
			\sum_{s'} P{(s' | s, a)} {[R{(s, a, s')} + \gamma \mi{max}_{a'} Q{(s', a')}]}
		\end{math}
	\end{parbox}
	\par

	La Q-function ottimale corrisponde alla Q-function calcolata a partire
	dalla politica che ne massimizza il valore di ricompensa atteso:

	\begin[mode = display]{math}
		Q*(s, a) = \mi{max}_{\pi} Q^{\pi} (s, a)
	\end{math}

	Questo significa che l'azione suggerita dalla politica ottimale
	\math{\pi*} nello stato \math{s} é data da:

	\begin[mode = display]{math}
		\pi*(s) = \mi{argmax}_{a} Q*(s, a)
	\end{math}

	Questo significa che per ricavare la politica ottimale \math{\pi*}
	di un MDP sarebbe necessario calcolare \math{Q(s, a)} per ciascuno
	stato \math{s} mediante l'equazione di Bellman. Indicando con \math{n}
	il numero di stati dell'MDP, ricavare \math{\pi*} significa risolvere
	un sistema di \math{n} equazioni in \math{n} incognite. Sebbene possibile,
	la presenza dell'operatore max rende il sistema non lineare, e quindi
	molto esoso da calcolare. É peró possibile ricavare la politica ottimale
	in tempo lineare mediante approssimazione.

	Uno fra questi approcci prende il nome di \strong{Value Iteration}.
	Sia \math{U_{i}(s)} il valore di utilitá per lo stato \math{s} alla
	\math{i}-esima iterazione, e sia \math{\epsilon} un parametro di
	terminazione. Viene chiamato \strong{aggiornamento di Bellman}
	l'aggiornamento di \math{U_{i}(s)} sulla base di \math{U_{i - 1}(s)}:

	\begin[mode = display]{math}
		U_{i} {(s)} \leftarrow \mi{max}_{a \in A(s)}
		\sum_{s'} T{(s, a, s')} [R{(s, a, s')} + \gamma U_{i - 1} {(s')}]
	\end{math}

	Ovvero, per tutte le azioni, viene calcolato il guadagno atteso sulla
	base dell'azione in questione ed ottenendo il risultato atteso.

	L'aggiornamento, ad ogni iterazione, viene compiuto per ciascuno stato.
	\math{U(s)} viene inizializzato ad un valore casuale (in genere 0) per
	ciascun \math{s \in S}, e le iterazioni proseguono fintanto che la
	differenza fra l'utilitá stimata fra una iterazione e quella successiva
	é superiore a \math{\epsilon} per almeno uno stato.

	L'aggiornamento di Bellman é una operazione approssimativamente
	lineare, avente tempo di esecuzione \math{O(S^{2}A)}. Naturalmente,
	occorre dimostrare che un numero sufficiente di iterazioni dell'algoritmo
	effettivamente restituiscano i valori di \math{U(s)} propri della politica
	ottimale.

	\begin{theorem}
		L'aggiornamento di Bellman é una contrazione.

		\bigskip
		\strong{Dimostrazione}. Per semplicitá, si consideri
		l'aggiornamento di Bellman come un operatore \math{B}.
		É quindi possibile scrivere:

		\begin[mode = display]{math}
			U_{i + 1} \leftarrow BU_{i}
		\end{math}

		Occorre definire una metrica per lo spazio dei vettori \math{U}.
		Sia \math{\abs{\abs{U}}} il valore assoluto della componente
		di \math{U} avente modulo maggiore:

		\begin[mode = display]{math}
			\abs{\abs{U}} = \mi{max}_{s} \abs{U(s)}
		\end{math}

		La metrica \math{d(U, U')} viene allora definita come
		\math{\abs{\abs{U - U'}}}, ovvero il valore assoluto
		della differenza fra le componenti aventi modulo
		maggiore delle due utilitá. Allora:

		\begin[mode = display]{math}
			\abs{\abs{BU - BU'}} \leq \gamma \abs{\abs{U - U'}}
		\end{math}

		Essendo \math{\gamma \in (0, 1)}, si ha che l'aggiornamento
		di Bellman é una contrazione rispetto al fattore \math{\gamma}
		e alla metrica \math{d}.
	\end{theorem}

	Una volta che \math{U_{i}(s)} é stato calcolato per tutti gli \math{s}
	con un \math{i} sufficientemente grande, ovvero quando \math{U_{i}(s)
	\approx U*(s)}, l'ultimo passo da compiere prevede di calcolare la
	politica \math{\pi*} sulla base di \math{U_{i}(s)}. L'operazione in
	questione prende il nome di \strong{policy extraction}:

	\begin[mode = display]{math}
		\pi*{(s)} \approx \pi_{U} {(s)} =
		\mi{argmax}_{a} \sum_{s'} P{(s' | a, s)} {[R{(s, a, s')}, \gamma U{(s')}]}
	\end{math}

	%%
	%% Altro esempio da prendere dal libro.
	%%

	Un approccio alternativo a value iteration é \strong{Policy
	Iteration}. L'approccio si basa sull'osservare che, anziché
	calcolare \math{U_{i}(s)} e poi dedurre la politica alla fine
	dell'algoritmo, é possibile calcolare direttamente le politiche
	approssimate alla fine di ciascuna iterazione.

	L'algoritmo prevede di ripetere in sequenza due step, chiamati
	\strong{Policy Evaluation} e \strong{Policy Improvement}, fino
	a quando la politica calcolata in una certa iterazione non é
	uguale alla politica calcolata all'iterazione precedente.

	Policy evaluation consiste nel calcolare, data una politica
	\math{\pi_{i}}, la funzione di utilitá in ciascuno stato se
	venisse applicata tale politica:

	\begin[mode = display]{math}
		U_{i} {(s)} \leftarrow \sum_{s'} P{(s' | s, \pi_{i - 1} {(s)})}
		{[R{(s, \pi_{i - 1} {(s)}, s')} + \gamma U_{i - 1} {(s')}]}
	\end{math}

	Policy improvement consiste nel calcolare una nuova politica
	\math{\pi_{i + 1}}, migliore di \math{\pi_{i}}, compiendo policy
	extraction su \math{U_{i}}:

	\begin[mode = display]{math}
		\pi_{i} {(s)} = \mi{argmax}_{a} \sum_{s'} P{(s' | s, a)}
		{[R{(s, a, s')} + \gamma U^{\pi_{i - 1}} {(s')}]}
	\end{math}

	L'algoritmo termina quando la politica \math{\pi_{i}} non é piú
	in grado di influire sul risultato di \math{U_{i}}. Quando questo
	accade, si ha che \math{U_{i}} é (approssimativamente) un punto
	fisso per l'aggiornamento di Bellman, ed é quindi una soluzione
	per l'equazione di Bellman, e la politica \math{\pi_{i}} che la
	ha generata é una politica ottima. Essendo il numero di politiche
	finito e venendo le politiche migliorate ad ogni iterazione, é
	garantito che l'algoritmo termini.

\end{sile}
