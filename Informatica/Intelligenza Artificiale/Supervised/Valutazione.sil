\begin{sile}

	Una volta costruito un modello per risolvere un problema
	di classificazione, si ha interesse a valutarne la qualita,
	testandolo con uno o piu dataset di prova. Questo puo essere
	fatto sia mediante delle metriche \strong{quantitative}, che
	forniscono un numero, che \strong{qualitative}, che forniscono
	una descrizione. E importante sottolineare che tali metriche
	si riferiscono esclusivamente ai test set, mai ai training
	set, perche altrimenti si avrebbe un evidente bias.

	Le principali metriche qualitative sono le seguenti:

	\begin{itemize}
		\begin{item}
			\strong{Efficienza}, ovvero sia il tempo di esecuzione
			necessario per la costruzione del modello (\strong{tempo
			di induzione}), sia il tempo di esecuzione impiegato dal
			modello nel venire utilizzato (\strong{tempo di inferenza});
		\end{item}
		\begin{item}
			\strong{Robustezza}, ovvero quanto bene il modello
			é in grado di gestire dati rumorosi (non cadere
			nell'overfitting) e se é in grado di gestire i dati
			mancanti (e come lo fa);
		\end{item}
		\begin{item}
			\strong{Scalabilitá}, ovvero quanto il modello riesce a
			contenere la crescita al crescere della dimensione dei
			dataset;
		\end{item}
		\begin{item}
			\strong{Interpretabilitá}, ovvero quanto il modello é
			in grado di "spiegare" il suo risultato a chi lo utilizza.
		\end{item}
		\begin{item}
			\strong{Compattezza}, ovvero quanto il modello riesce
			a descrivere il dataset in maniera conservativa (senza 
			componenti ridondanti);
		\end{item}
	\end{itemize}

	\bigskip

	Per trattare le metriche quantitative e prima necessario
	introdurre la seguente matrice, detta \strong{matrice di
	confusione}:

	\begin[cols = 20%fw 40%fw 40%fw, cellborder = 0]{ptable}
		\begin{row}
			\begin{cell}
			\end{cell}
			\begin{cell}
				Classificato come positivo
			\end{cell}
			\begin{cell}
				Classificato come negativo
			\end{cell}
		\end{row}
		\begin{row}
			\begin{cell}
				Effettivamente positivo
			\end{cell}
			\begin{cell}
				TP (\strong{True Positive}): il numero di esempi
				positivi \par classificati correttamente
			\end{cell}
			\begin{cell}
				FN (\strong{False Negative}): il numero di esempi
				positivi \par classificati erroneamente
			\end{cell}
		\end{row}
		\begin{row}
			\begin{cell}
				Effettivamente negativo
			\end{cell}
			\begin{cell}
				FP (\strong{False Positive}): il numero di esempi
				negativi \par classificati erroneamente
			\end{cell}
			\begin{cell}
				TN (\strong{True Negative}): il numero di esempi
				negativi \par classificati correttamente
			\end{cell}
		\end{row}
	\end{ptable}
	\smallskip

	La metrica quantitativa piu intuitiva e l'\strong{accuratezza
	predittiva} \math{A}, ovvero il rapporto fra il numero di
	classificazioni corrette (sia positive che negative) ed il
	numero di elementi del dataset usato per il testing:

	\begin[mode = display]{math}
		A = \frac{TP + TN}{TP + FP + TN + FN} = 
		\frac{\mi{Numero} \thickspace \mi{di} \thickspace
		\mi{classificazioni} \thickspace \mi{corrette}}{\mi{Numero}
		\thickspace \mi{totale} \thickspace \mi{di} \thickspace
		\mi{elementi} \thickspace \mi{del} \thickspace \mi{test}
		\thickspace \mi{set}}
	\end{math}

	Naturalmente, l'accuratezza predittiva é tanto maggiore
	quanto piú il suo valore tende ad 1.

	L'accuratezza predittiva e piuttosto omnicomprensiva, nel senso
	che riassume tutti gli aspetti del dataset in un solo numero.
	Talvolta, si preferisce avere metriche quantitative che si
	riferiscono ai soli esempi positivi. A tale scopo, e possibile
	definire le metriche \strong{precision} \math{p} (anche detta
	\strong{sensitivity}) e \strong{recall} \math{r}:

	\begin[width = 50%fw]{parbox}
		\begin[mode = display]{math}
			p = \frac{TP}{TP + FP} =
			\frac{\mi{Esempi} \thickspace \mi{positivi}
			\thickspace \mi{classificati} \thickspace
			\mi{correttamente}}{\mi{Esempi} \thickspace
			\mi{classificati} \thickspace \mi{positivi}}
		\end{math}
	\end{parbox}
	\begin[width = 50%fw]{parbox}
		\begin[mode = display]{math}
			r = \frac{TP}{TP + FN} =
			\frac{\mi{Esempi} \thickspace \mi{positivi}
			\thickspace \mi{classificati} \thickspace
			\mi{correttamente}}{\mi{Esempi} \thickspace
			\mi{effettivamente} \thickspace \mi{positivi}}
		\end{math}
	\end{parbox}
	\par

	\math{p} rappresenta quanto bene il modello é in grado di
	classificare correttamente gli esempi positivi (tralasciando
	i falsi negativi), mentre \math{r} rappresenta quanto il modello
	é in grado di "coprire" i dati (quanto poco tralascia gli esempi
	positivi, anche a costo di commettere un errore).

	Per comoditá, é possibile combinare le due metriche
	in una sola, chiamata \strong{F\textsubscript{1}-value} (o
	\strong{F\textsubscript{1}-score}), che non é altro che la
	loro media armonica:

	\begin[mode = display]{math}
		F_{1} = {(\frac{p^{-1} + r^{-1}}{2})}^{-1} =
		\frac{2}{p^{-1} + r^{-1}} =
		\frac{2}{\frac{1}{p} + \frac{1}{r}} =
		\frac{2pr}{p + r}
	\end{math}

	Questa metrica é di particolare interesse perché la media
	armonica di due valori tende ad essere vicina al piú piccolo
	dei due. Inoltre, dato che \math{p} e \math{q} compaiono sia
	al numeratore che al denominatore, il valore di \math{F_{1}}
	é grande solamente se sia \math{p} che \math{q} sono a loro
	volta grandi.

	Un'ulteriore metrica e la cosiddetta \strong{specificity}
	\math{s}, definita come il rapporto fra il numero di esempi
	effettivamente negativi ed il numero totale di esempi
	classificati negativi (sia correttamente che non
	correttamente):

	\begin[mode = display]{math}
		s = \frac{TN}{TN + FP} = \frac{\mi{Esempi} \thickspace
		\mi{effettivamente} \thickspace \mi{negativi}}{\mi{Esempi}
		\thickspace \mi{classificati} \thickspace \mi{negativi}}
	\end{math}

	Questa e la "controparte" di \math{p} rispetto agli esempi
	negativi.

\end{sile}
