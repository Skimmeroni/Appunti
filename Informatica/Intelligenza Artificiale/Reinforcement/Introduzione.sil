\begin{sile}

	Nel \strong{reinforcement learning}, l'agente interagisce
	con l'ambiente e riceve periodicamente delle ricompense o
	delle punizioni, sotto forma di \strong{segnali}, che
	riflettono il modo in cui sta operando. Non é presente
	alcun supervisore. In genere il \strong{feedback}, ovvero
	la reazione dell'agente al segnale, non é immediata, ma
	impiega del tempo per essere elaborata. Possono presentarsi
	delle situazioni in cui il guadagno sul breve termine deve
	essere sacrificato per ottenere un guadagno sul lungo termine.

	La ricompensa, indicata con \math{R_{t}}, è un segnale di
	feedback scalare che indica quanto bene si sta comportando
	l'agente all'istante di tempo \math{t}. Lo scopo dell'agente
	è massimizzare la stima della ricompensa cumulativa che si
	guadagnerà da ora fino al raggiungimento dell'obiettivo. Questo
	sottostà ad un assunto, la \strong{reward hypothesis}, ovvero
	che tutti gli obiettivi possano essere espressi come
	massimizzazione della ricompensa cumulativa attesa. Si noti 
	come questo assunto non solo non sia sempre valido, possibile,
	ma spesso ha anche un risultato non neutrale.

	Ad ogni istante \math{t}, l'agente esegue una certa azione
	\math{A_{t}}, percepisce l'osservazione \math{O_{t}} e riceve
	la ricompensa \math{R_{t}}. D'altro canto, l'ambiente riceve
	l'azione \math{A_{t}}, emette l'osservazione \math{O_{t + 1}}
	ed emette la ricompensa \math{R_{t + 1}}. \math{t} viene
	incrementato di 1 ed il ciclo si ripete. Una sequenza ordinata
	di azioni, osservazioni e ricompense prende il nome di
	\strong{storia}:

	\begin[mode = display]{math}
		H_{t} = A_{1}, O_{1}, R_{1}, \unicodeellipsis,
				A_{t - 1}, O_{t - 1}, R_{t - 1},
				A_{t}, O_{t}, R_{t}
	\end{math}

	Dove \math{H_{t}} indica la storia dal primo istante all'istante
	\math{t}. Cosa accadrà all'istante \math{t + 1} dipende in qualche
	misura da \math{H_{t}}; nello specifico, si usa il termine
	\strong{stato} per indicare una funzione della storia che
	determina il futuro operato dell'agente sulla base della stessa:

	\begin[mode = display]{math}
		S_{t} = f(H_{t})
	\end{math}

	Essendo i problemi di reinforcement learning intrinsecamente
	sequenziali, é in genere preferibile una situazione in cui
	vale per gli stati l'assunzione Markoviana:

	\begin[mode = display]{math}
		P(S_{t + 1} | S_{t}, S_{t - 1}, \unicodeellipsis, S_{1}) =
		P(S_{t + 1} | S_{t})
	\end{math}

	Ovvero, la probabilità che ci si trovi nello stato \math{S_{t + 1}}
	non dipende da tutti dagli stati a questo precedenti, ma soltanto
	da quello immediatamente precedente.

	Occorre fare una distinzione fra lo \strong{stato dell'ambiente}
	e lo \strong{stato dell'agente}. Il primo è la rappresentazione
	interna dell'ambiente, che usa per fornire all'agente
	l'osservazione e la ricompensa, e non è necessariamente noto
	all'agente (anche se lo fosse, potrebbe essere molte più
	informazioni di quanto l'agente necessiti). Il secondo è la
	rappresentazione interna dell'agente, che questo usa per 
	scegliere l'azione da compiere in ciascun istante.

	Nel caso in cui i due stati coincidano, ovvero nel caso in cui
	l'ambiente è accessibile, il reinforcement learning può essere
	formulato come un Markov Decision Process. Nel caso in cui
	l'ambiente non sia accessibile, i due non coincidono, e si
	parla di \strong{Partially Observable Markov Decision Process}
	(\strong{POMDP}), dove l'agente necessita di costruire una
	rappresentazione interna dei propri stati.

\end{sile}
