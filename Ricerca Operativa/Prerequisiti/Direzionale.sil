\begin{sile}

	Sia data una funzione \math{f : X \subseteq \dsi{R}^{n} \mapsto 
	\dsi{R}}. Siano poi \math{\bi{v} \in \dsi{R}} un vettore non 
	nullo e \math{\bi{x}_{0} \in X^{o}} un punto interno ad \math{X}. 
	Si dice \strong{derivata nella direzione} \math{\bi{v}} di \math{f} 
	nel punto \math{x_{0}} il valore, ammesso che esista, del seguente 
	limite:

	\begin[mode = display]{math}
		D_{\bi{v}} f{(\bi{x}_{0})} =
		\frac{\partial f}{\partial \bi{v}} =
		\mi{lim}_{t \rightarrow 0} \frac{f(\bi{x}_{0} + t\bi{v}) - f(\bi{x}_{0})}{t}
	\end{math}

	La derivata direzionale esprime la pendenza della funzione. Derivate 
	direzionali di particolare rilevanza sono quelle valutate rispetto agli 
	assi. Se il vettore \math{\bi{v}} rispetto alla quale si calcola la derivata 
	parziale é uno dei versori degli assi (ovvero, uno dei vettori della base 
	canonica di \math{\dsi{R}^{n}}), la derivata direzionale rispetto a questo 
	vettore viene chiamata \strong{derivata parziale}, e viene indicata con 
	\math{\frac{\partial f}{\partial x_{i}}}, dove \math{x_{i}} indica la 
	\math{i}-esima componente della base canonica di \math{\dsi{R}_{n}}.
	Una funzione \math{f : X \subseteq \dsi{R}^{n} \mapsto \dsi{R}} si dice
	\strong{derivabile} nel punto \math{\bi{x}_{0} \in X} se tutte le sue derivate 
	parziali in \math{x_{0}} sono definite.

	La derivata direzionale (e parziale) per funzioni su \math{\dsi{R}^{n}} 
	mantiene tutte le proprietá algebriche (linearitá, regola della catena, 
	derivate notevoli, ecc\math{\unicodeellipsis}) della derivata per funzioni
	ad una variabile. Il calcolo della \math{i}-esima derivata parziale é molto 
	semplice, dato che é sufficiente derivate la funzione normalmente considerando 
	tutte le componenti al di fuori della \math{i}-esima come costanti.

	\begin{example}
		\begin[width = 33%fw]{parbox}
			\begin[mode = display]{math}
				f(x, y) = x^{2} \mi{cos}(y)
			\end{math}
		\end{parbox}
		\begin[width = 33%fw]{parbox}
			\begin[mode = display]{math}
				\frac{\partial f}{\partial x} {(x^{2} \mi{cos}(y))} =
				2x \mi{cos} {(y)}
			\end{math}
		\end{parbox}
		\begin[width = 33%fw]{parbox}
			\begin[mode = display]{math}
				\frac{\partial f}{\partial y} {(x^{2} \mi{cos}(y))} =
				-x^{2} \mi{sin} {(y)}
			\end{math}
		\end{parbox}
	\end{example}

	Data una funzione \math{f : X \subseteq \dsi{R}^{n} \mapsto
	\dsi{R}}, viene chiamato \strong{vettore gradiente}, indicato
	con il simbolo \math{\nabla f} oppure come \math{\mi{grad} f},
	il vettore \math{n}-dimensionale che ha per componenti le \math{n}
	derivate parziali di \math{f}. Rispetto al dominio della funzione
	nel suo complesso, il vettore gradiente é un campo vettoriale,
	perché il suo valore non é necessariamente lo stesso per ogni
	punto in cui la funzione é definita. 

	\begin[mode = display]{math}
		\nabla f{(\bi{x})} =
		\table{
			\frac{\partial f(\bi{x})}{\partial x_{1}} &
			\frac{\partial f(\bi{x})}{\partial x_{2}} &
			\unicodeellipsis &
			\frac{\partial f(\bi{x})}{\partial x_{n}} \\
		}
	\end{math}

	\begin{theorem}
		Se una funzione ha un massimo o un minimo locale in un punto interno al suo 
		dominio e la derivata direzionale della funzione in quel punto esiste, allora
		é nulla.

		\bigskip
		\strong{Dimostrazione}. Siano date una funzione \math{f: X \subseteq 
		\dsi{R}^{n} \mapsto \dsi{R}} ed un vettore \math{\bi{v} \ne 0 
		\in \dsi{R}^{n}}. Si assuma che \math{f} abbia un massimo o un minimo 
		locale nel punto \math{\bi{x}_{0} \in X^{o}}.

		Sia \math{g(t) = f(\bi{x}_{0} + t\bi{v})} una funzione a una variabile 
		costruita a partire da \math{f}. Per \math{t} piccolo, la funzione \math{g} 
		é certamente ben definita. Per funzioni ad una variabile é noto che se questa
		ha un massimo o un minimo locale in un punto ed é derivabile in quel punto, 
		allora la derivata in quel punto deve essere nulla. Pertanto:

		\begin[mode = display]{math}
			g'{(t)} = \frac{dg}{dt} {(t)} = \mi{lim}_{t \rightarrow 0} \frac{g(t) - 
			g(0)}{t} = \mi{lim}_{t \rightarrow 0} \frac{f(\bi{x}_{0} + t\bi{v}) - 
			f(\bi{x}_{0})}{t} = D_{\bi{v}} f{(\bi{x}_{0})} = 0
		\end{math}

		Che era il risultato che si stava cercando.
	\end{theorem}

	\begin{theorem}
		Se una funzione \math{f : X \subseteq \dsi{R}^{n} \mapsto \dsi{R}} ha 
		derivata direzionale nulla nel punto \math{\bi{x}_{0}}, allora anche 
		il gradiente in \math{x_{0}} é nullo.

		\bigskip
		\strong{Dimostrazione}. Questo segue direttamente dal teorema appena mostrato:
		se la derivata direzionale é nulla per un vettore generico, allora lo 
		saranno anche le derivate direzionali rispetto alla base canonica di 
		\math{\dsi{R}^{n}}, ovvero le derivate parziali.
	\end{theorem}

	I punti di una funzione che hanno gradiente nullo si dicono \strong{punti di 
	equilibrio} oppure \strong{punti stazionari} oppure \strong{punti critici}. Il teorema
	appena mostrato fornisce un modo semplice per trovare i punti stazionari di 
	una funzione: é sufficiente calcolarne il vettore (campo) gradiente e valutare
	i punti, se esistono, in cui questo si annulla.

	\begin{example}
		Data la funzione \math{x(1 - x^{2} - 4y^{2})} ristretta al dominio \math{X = 
		\{x^{2} + 4y^{2} < 1\}}, si é interessati a studiarne i punti stazionari. 
		Si calcolino le derivate parziali:

		\begin[mode = display]{math}
			\{\table[columnalign = left]{
				\frac{\partial f}{\partial x} {(x(1 - x^{2} - 4y^{2}))} = 
				x{(\frac{\partial f}{\partial x} {(1 - x^{2} - 4y^{2})})} +
				{(1 - x^{2} - 4y^{2})} {(\frac{\partial f}{\partial x} {(x)})} = 
				1 - 3x^{2} - 4y^{2} \\
				\frac{\partial f}{\partial y} {(x(1 - x^{2} - 4y^{2}))} = 
				{(1 - x^{2} - 4y^{2})} {(\frac{\partial f}{\partial y} {(x)})} + 
				x {(\frac{\partial f}{\partial y} {(1 - x^{2} - 4y^{2})})} =
				-8xy \\
			}
		\end{math}

		Queste corrispondono alle componenti del vettore (campo) gradiente. Imponendo
		che siano entrambe nulle, si ha:

		\begin[mode = display]{math}
			\{\table[columnalign = left]{
				1 - 3x^{2} - 4y^{2} = 0 \\
				-8xy = 0 \\
			}
			\thickspace\Rightarrow\thickspace
			\{\table[columnalign = left]{
				1 - 0 - 4y^{2} = 0 \\
				x = 0 \\
			}
			\thickspace\cup\thickspace
			\{\table[columnalign = left]{
				1 - 3x^{2} - 0 = 0 \\
				y = 0 \\
			}
			\thickspace\Rightarrow\thickspace
			\{\table[columnalign = left]{
				y = \frac{1}{2} \\
				x = 0 \\
			}
			\thickspace\cup\thickspace
			\{\table[columnalign = left]{
				x = \frac{1}{\sqrt 3} \\
				y = 0 \\
			}
		\end{math}

		Si ottengono le due coppie di punti \math{(0, \frac{1}{2})} e 
		\math{(\frac{1}{\sqrt 3}, 0)}. Si noti peró come:

		\begin[width = 50%fw]{parbox}
			\begin[mode = display]{math}
				{(0)}^{2} + 4{(\frac{1}{2})}^{2} < 1
				\thickspace\Rightarrow\thickspace
				1 < 1
			\end{math}
		\end{parbox}
		\begin[width = 50%fw]{parbox}
			\begin[mode = display]{math}
				{(\frac{1}{\sqrt 3})}^{2} + 4{(0)}^{2} < 1
				\thickspace\Rightarrow\thickspace
				\frac{1}{3} < 1
			\end{math}
		\end{parbox}
		\par

		Pertanto, solamente la coppia \math{(\frac{1}{\sqrt 3}, 0)} é un punto
		stazionario per \math{X}.
	\end{example}

	Le derivate parziali di una funzione, essendo funzioni esse stesse, possono 
	essere a loro volta derivate. Data la derivata parziale \math{\partial f / 
	\partial i} di una funzione \math{f} ad \math{n} componenti rispetto alla 
	componente \math{i}, la sua derivata parziale rispetto a \math{j} viene 
	espressa come \math{\partial f \partial f / \partial j \partial i}, o piú 
	semplicemente \math{\partial^{2} f / \partial j \partial i}. Nel caso in 
	cui \math{i = j}, é possibile scrivere la derivata parziale della derivata 
	parziale nella forma ancora piú compatta \math{\partial^{2} f / \partial^{2} 
	i}. Nel caso piú generale possibile:

	\begin[width = 50%fw]{parbox}
		\begin[mode = display]{math}
			\frac{\partial^{i} f}{\partial j} {(\frac{\partial f}{\partial x \partial y 
			\unicodeellipsis \partial z})} = \frac{\partial^{i + 1} f}{\partial j 
			\partial x \partial y \unicodeellipsis \partial z}
		\end{math}
	\end{parbox}
	\begin[width = 50%fw, strut = character, valign = middle]{parbox}
		Con \math{x, y, z, j} non necessariamente distinti
	\end{parbox}
	\par

	Nel caso particolare di una funzione \math{f} scalare a due variabili, le 
	derivate parziali \math{\frac{\partial^{2} f}{\partial x \partial y}} e 
	\math{\frac{\partial^{2} f}{\partial y \partial x}} sono dette \strong{derivate 
	miste}.

	\begin{theorem}
		\strong{Teorema delle derivate miste}. Se una funzione a due variabili ha definite
		le derivate parziali miste nell'intorno di un punto e sono entrambe continue 
		in quel punto, allora le due derivate miste nel punto coincidono:

		\begin[mode = display]{math}
			\frac{\partial^{2} f}{\partial x \partial y} {(\bi{x}_{0}, \bi{y}_{0})} = 
			\frac{\partial^{2} f}{\partial y \partial x} {(\bi{x}_{0}, \bi{y}_{0})}
		\end{math}
	\end{theorem}

	Si dice \strong{matrice hessiana} di una funzione \math{f : X \subseteq 
	\dsi{R}^{n} \mapsto \dsi{R}} la matrice \math{H(f(\bi{x}))} costruita
	disponendo ordinatamente le derivate parziali seconde della funzione
	rispetto alle \math{n} variabili:

	\begin[mode = display]{math}
		H(f(\bi{x})) = [\table{
			\frac{\partial^{2} f}{\partial^{2} x_{1}} &
			\frac{\partial^{2} f}{\partial x_{1} \partial x_{2}} &
			\unicodecdots &
			\frac{\partial^{2} f}{\partial x_{1} \partial x_{n}} \\
			\frac{\partial^{2} f}{\partial x_{2} \partial x_{1}} &
			\frac{\partial^{2} f}{\partial^{2} x_{2}} &
			\unicodecdots &
			\frac{\partial^{2} f}{\partial x_{2} \partial x_{n}} \\
			\vdots &
			\vdots &
			\ddots &
			\vdots \\
			\frac{\partial^{2} f}{\partial x_{n} \partial x_{1}} &
			\frac{\partial^{2} f}{\partial x_{n} \partial x_{2}} &
			\unicodecdots &
			\frac{\partial^{2} f}{\partial^{2} x_{n}} \\
		}]
	\end{math}

	Le posizioni delle derivate parziali seconde \math{\partial^{2} f / \partial 
	x_{i} \partial x_{j}} e \math{\partial^{2} f / \partial x_{j} \partial x_{i}} 
	siano, rispettivamente, \math{(i, j)} e \math{(j, i)} per \math{i \ne j}.
	Ricordando che se é valido il teorema delle derivate miste allora 
	\math{\partial^{2} f / \partial x_{i} \partial x_{j} = \partial^{2} f / 
	\partial x_{j} \partial x_{i}}, questo significa che la matrice hessiana di 
	una funzione per la quale il teorema é valido é una matrice simmetrica.

	\begin{example}
		\begin[width = 35%fw]{parbox}
			\begin[mode = display]{math}
				f(x, y) = x^{2} + 16y^{2} + 6xy
			\end{math}
		\end{parbox}
		\begin[width = 15%fw]{parbox}
			\begin[mode = display]{math}
				\nabla f(x, y) =
				\table[columnalign = left]{
					2x \\
					32y \\
				}
			\end{math}
		\end{parbox}
		\begin[width = 50%fw]{parbox}
			\begin[mode = display]{math}
				H {(f{(x, y)})} = 
				{[\table[columnalign = center center]{
					2 & 0 \\
					0 & 32 \\
				}]}
			\end{math}
		\end{parbox}
	\end{example}

	Data una funzione \math{f(x)}, siano \math{x_{p}} e \math{x_{q}} due
    punti appartenenti all'intervallo \math{[a, b] \subseteq \mi{Dom}(f(x))}
    tali per cui \math{x_{p} < x_{q}}. Sia poi \math{\lambda} un qualsiasi
    valore strettamente compreso fra 0 e 1. Se vale la disuguaglianza in
    basso a sinistra, la funzione \math{f(x)} é detta \strong{convessa
    nell'intervallo [a, b]}; se vale invece quella in basso a destra, é
    detta \strong{concava nell'intervallo [a, b]}:

    \begin[width = 50%fw]{parbox}
        \begin[mode = display]{math}
            f(\lambda x_{q} + (1 - \lambda) x_{p}) \leq
            \lambda f(x_{q}) + (1 - \lambda) f(x_{p})
        \end{math}
    \end{parbox}
    \begin[width = 50%fw]{parbox}
        \begin[mode = display]{math}
            f(\lambda x_{q} + (1 - \lambda) x_{p}) \geq
            \lambda f(x_{q}) + (1 - \lambda) f(x_{p})
        \end{math}
    \end{parbox}
    \par

    Le definizioni di funzione concava e funzione convessa in un intervallo
    non sono mutualmente esclusive. Infatti, una funzione potrebbe non essere
    né concava né convessa in un intervallo cosí come essere sia concava sia
    convessa in un intervallo.

    Piú in generale, una funzione viene semplicemente detta \strong{funzione
    convessa} se é convessa su tutto il suo dominio; allo stesso modo, una
    funzione viene semplicemente detta \strong{funzione concava} se é concava
    su tutto il suo dominio.

    \begin{theorem}
        \strong{Criterio di concavitá e convessitá per funzioni ad
        una variabile}. Una funzione ad una variabile due volte derivabile
        é convessa in un intervallo se la sua derivata seconda é nulla o
        negativa in tutti i punti di tale intervallo. Similmente, tale
        funzione é concava in un intervallo se la sua derivata seconda é
        nulla o positiva in tutti i punti di tale intervallo.
    \end{theorem}

    Si noti come la non esistenza della derivata seconda di una funzione
    ad una variabile in un intervallo non implichi l'impossibilitá di
    determinare in toto se questa sia concava o convessa in tale intervallo.
    Infatti, sebbene il criterio sopra presentato non sia applicabile, la
    definizione di concavitá/convessitá potrebbe essere comunque rispettata.

    Sostituendo i punti \math{x_{p}} e \math{x_{q}} nelle definizioni
    di concavitá e convessitá con i vettori \math{n}-dimensionali
    \math{\bi{x}_{p}} e \math{\bi{x}_{q}}, si estendono tali definizioni
    alle funzioni a piú di una variabile:

    \begin[width = 50%fw]{parbox}
        \begin[mode = display]{math}
            f(\lambda \bi{x}_{q} + (1 - \lambda) \bi{x}_{p}) \leq
            \lambda f(\bi{x}_{q}) + (1 - \lambda) f(\bi{x}_{p})
        \end{math}
    \end{parbox}
    \begin[width = 50%fw]{parbox}
        \begin[mode = display]{math}
            f(\lambda \bi{x}_{q} + (1 - \lambda) \bi{x}_{p}) \leq
            \lambda f(\bi{x}_{q}) + (1 - \lambda) f(\bi{x}_{p})
        \end{math}
    \end{parbox}
    \par

    \begin{theorem}
        \strong{Criterio di concavitá e convessitá per funzioni a piú
        variabili}.

		Sia data una funzione \math{f : X \subseteq \dsi{R}^{n} \mapsto
		\dsi{R}} per la quale esistono e sono definite tutte le derivate
		parziali seconde in un intervallo. Sia poi \math{H(f(\bi{x}))} la
		matrice Hessiana a questa associata. Si ha allora:

		\begin{itemize}
			\begin{item}
				Se \math{H(f(\bi{x}))} é definita positiva per tutti i
				punti che appartengono all'intervallo, allora \math{f} é
				convessa in tale intervallo;
			\end{item}
			\begin{item}
				Se \math{H(f(\bi{x}))} é definita negativa per tutti i
				punti che appartengono all'intervallo, allora \math{f} é
				concava in tale intervallo;
			\end{item}
			\begin{item}
				Se \math{f} é convessa in tutti i punti che appartengono
				all'intervallo, allora \math{H(f(\bi{x}))} é semidefinita
				positiva per tutti i punti dell'intervallo;
			\end{item}
			\begin{item}
				Se \math{f} é concava in tutti i punti che appartengono
				all'intervallo, allora \math{H(f(\bi{x}))} é semidefinita
				negativa per tutti i punti dell'intervallo.
			\end{item}
		\end{itemize}
    \end{theorem}

    \begin{example}
        Data la funzione \math{f(x, y) = x^{2} - 2xy + y^{2}}, se ne calcoli
        la matrice Hessiana:

        \begin[mode = display]{math}
            H =
            {[\table[columnalign = left left]{
                \frac{\partial^{2} f(x, y)}{\partial x^{2}} &
                \frac{\partial^{2} f(x, y)}{\partial x \partial y} \\
                \frac{\partial^{2} f(x, y)}{\partial y \partial x} &
                \frac{\partial^{2} f(x, y)}{\partial y^{2}} \\
            }]}
            =
            {[\table[columnalign = left left]{
               \frac{\partial}{\partial x} {(\frac{\partial}{\partial x} {(x^{2} - 2xy + y^{2})})} &
               \frac{\partial}{\partial y} {(\frac{\partial}{\partial x} {(x^{2} - 2xy + y^{2})})} \\
               \frac{\partial}{\partial x} {(\frac{\partial}{\partial y} {(x^{2} - 2xy + y^{2})})} &
               \frac{\partial}{\partial y} {(\frac{\partial}{\partial y} {(x^{2} - 2xy + y^{2})})} \\              
            }]}
            =
            {[\table[columnalign = left left]{
               \frac{\partial}{\partial x} {(2x - 2y)} &
               \frac{\partial}{\partial y} {(2x - 2y)} \\
               \frac{\partial}{\partial x} {(-2x + 2y)} &
               \frac{\partial}{\partial y} {(-2x + 2y)}  \\
            }]}
            =
            {[\table[columnalign = right right]{
                2 & -2 \\
               -2 & 2 \\
            }]}
        \end{math}

        Tale matrice é semidefinita positiva. Infatti:

        \begin[mode = display]{math}
            z^{T}Hz =
            {[\table{x & y \\}]}
            {[\table[columnalign = right right]{2 & -2 \\ -2 & 2 \\}]}
            {[\table{x \\ y \\}]} =
            {[\table{2x - 2y & -2x + 2y \\}]}
            {[\table{x \\ y \\}]} =
            {(2x - 2y)}x + {(-2x + 2y)}y =
            2x^{2} - 4xy + 2y^{2} =
            2{(x - y)}^{2}
        \end{math}

        Che é una quantitá non negativa per qualsiasi valore di \math{x}
        e di \math{y}. La funzione \math{f(x, y)} é allora una funzione
        convessa.
    \end{example}

    \begin{theorem}
        Se \math{f(x_{1}, x_{2}, \unicodeellipsis, x_{n})} é una funzione
        convessa, allora \math{-f(x_{1}, x_{2}, \unicodeellipsis, x_{n})}
        é una funzione concava, e viceversa.

        \bigskip
        \strong{Dimostrazione}. Sia \math{f(x_{1}, x_{2}, \unicodeellipsis,
        x_{n})} una funzione convessa. Allora, per qualsiasi \math{\bi{x}_{p},
        \bi{x}_{q} \in \mi{Dom}(f)} e per qualsiasi \math{\lambda \in (0, 1)},
        deve valere:

        \begin[mode = display]{math}
            f(\lambda x_{q} + (1 - \lambda) x_{p}) \leq
            \lambda f(x_{q}) + (1 - \lambda) f(x_{p})
        \end{math}

        Moltiplicando ambo i membri per -1, si ha:

        \begin[mode = display]{math}
            -(f(\lambda x_{q} + (1 - \lambda) x_{p})) \geq
            -(\lambda f(x_{q}) + (1 - \lambda) f(x_{p}))
            \thickspace \Rightarrow \thickspace
            -f(\lambda x_{q} + (1 - \lambda) x_{p}) \geq
            \lambda (-f(x_{q})) + (1 - \lambda) (-f(x_{p}))
        \end{math}

        Che é la definizione di funzione concava per \math{-f(x_{1}, x_{2},
        \unicodeellipsis, x_{n})}. La dimostrazione in senso inverso é
        sostanzialmente analoga.
    \end{theorem}

    \begin{theorem}
        La somma di piú funzioni convesse é una funzione convessa; la somma
        di piú funzioni concave é una funzione concava.
    \end{theorem}

    \begin{theorem}
        I massimi locali di una funzione convessa sono anche massimi globali;
        I minimi locali di una funzione concava sono anche minimi globali.
    \end{theorem}

    \begin{theorem}
        Una funzione lineare é sempre sia concava sia convessa.

        \bigskip
        \strong{Dimostrazione}. La derivata seconda di una funzione lineare
        generica ad una variabile \math{ax + b} é sempre nulla:

        \begin[mode = display]{math}
            \frac{d^{2}f}{dx} {(ax + b)} =
            \frac{df}{dx} {(\frac{df}{dx} {(ax + b)})} =
            \frac{df}{dx} {(\frac{df}{dx} {(ax)} + \frac{df}{dx} {(b)})} =
            \frac{df}{dx} {(a + 0)} = 0
        \end{math}

        Pertanto, per definizione, la sua derivata seconda é sempre sia
        positiva o nulla (quindi é convessa) e sempre sia negativa o nulla
        (quindi é concava). 
    \end{theorem}

	\begin{theorem}
		Sia data una funzione \math{f : X \subseteq \dsi{R}^{n} \mapsto
		\dsi{R}}, sia \math{\bi{x}_{0}} un suo punto critico. Sia poi
		\math{H(f(\bi{x}_{0}))} la matrice Hessiana di \math{f} valutata
		in tale punto. Si ha allora:

		\begin{itemize}
			\begin{item}
				Se \math{H(f(\bi{x}_{0}))} é definita positiva, allora
				\math{\bi{x}_{0}} é un punto di minimo;
			\end{item}
			\begin{item}
				Se \math{H(f(\bi{x}_{0}))} é definita negativa, allora
				\math{\bi{x}_{0}} é un punto di massimo;
			\end{item}
			\begin{item}
				Se gli autovalori di \math{H(f(\bi{x}_{0}))} hanno
				segni discordi e nessuno di questi é nullo, allora
				\math{\bi{x}_{0}} é un punto sella.
			\end{item}
		\end{itemize}
	\end{theorem}

\end{sile}
