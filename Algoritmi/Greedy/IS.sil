\begin{sile}

	Sia dato un insieme \math{A = \{a_{1}, a_{2}, \unicodeellipsis,
	a_{n}\}} costituito da "attività". Ciascuna attività \math{a_{i}}
	è definita come una coppia \math{(s_{i}, f_{i})}, dove i due valori
	indicano rispettivamente il tempo di inizio dell'attività ed il
	tempo di fine.

	Due attività \math{a_{i}} e \math{a_{j}} distinte si dicono
	\em{compatibili} se \math{a_{i}} termina prima che \math{a_{j}}
	inizi, ovvero se \math{f_{i} \leq s_{j}}. Più in generale, un 
	sottoinsieme \math{X} di \math{A} è detto \em{compatibile} se 
	è costituito da attività fra di loro mutualmente compatibili,
	o equivalentemente se non contiene alcuna coppia di elementi
	di \math{A} fra di loro non compatibili.

	\begin[width = 45%fw]{parbox}
		\begin[mode = display]{math}
			\mi{Comp} {(i, j)} = 
			\{\table{
				T & \mi{se} \thickspace f_{i} \leq s_{j} \\
				F & \mi{se} \thickspace f_{i} > s_{j} \\
			}
		\end{math}
	\end{parbox}
	\begin[width = 55%fw]{parbox}
		\begin[mode = display]{math}
			\mi{Comp} {(X)} = 
			\{\table{
				T & \mi{se} \thickspace \nexists i, j \in X \thickspace
										\mi{t.c.} \thickspace f_{i} > s_{j} \\
				F & \mi{se} \thickspace \exists i, j \in X \thickspace
										\mi{t.c.} \thickspace f_{i} > s_{j} \\
			}
		\end{math}
	\end{parbox}
	\par

	Il problema \strong{Interval Scheduling} chiede di trovare il
	sottoinsieme \math{X} di \math{A} di cardinalità massima composto
	da attività mutualmente compatibili \footnote{Si noti la differenza
	con il problema Weighted Interval Scheduling: tale problema chiede di
	trovare il sottoinsieme di valore totale massimo, mentre il problema
	Interval Scheduling chiede di trovare quello di cardinalità massima.}.

	Il problema Interval Scheduling è risolvibile mediante programmazione
	dinamica. Dato un insieme \math{A = \{a_{1}, a_{2}, \unicodeellipsis,
	a_{n}\}} di attività, si ordinino tali attività per tempo di fine non
	decrescente. Affinché un algoritmo di programmazione dinamica possa 
	essere applicabile è necessario aggiungere due attività "slack",
	\math{a_{0}} e \math{a_{n + 1}}, rispettivamente in prima ed in
	ultima posizione. Sia allora \math{A' = A \cup \{a_{0}, a_{n + 1}\}}.

	Sia \math{X_{i,j}} il sottoinsieme di \math{A'} composto da tutte le
	attività che vengono dopo \math{a_{i}} e prima di \math{a_{j}}, ovvero
	\math{X_{i, j} = \langle a_{i + 1}, a_{i + 2}, \unicodeellipsis, a_{j -
	2}, a_{j - 1} \rangle}. La soluzione \math{S_{i, j}} per la \math{i,
	j}-esima istanza del problema corrisponde a trovare il sottoinsieme
	di attività mutualmente compatibili di cardinalità massima rispetto
	al sottoinsieme \math{X_{i, j}}. La cardinalità del sottoinsieme di
	attività mutualmente compatibili di cardinalità massima per la
	\math{i, j}-esima istanza del problema viene indicata con
	\math{\mi{Opt}(i, j)}.

	La soluzione ottimale per il problema principale è data da
	\math{S_{0, n + 1}}. Infatti, tale insieme è la soluzione
	per l'insieme che contiene gli elementi di \math{A'} che
	vengono dopo \math{a_{0}} e prima di \math{a_{n + 1}}, che
	corrisponde esattamente a \math{A' - \{a_{0}, a_{n + 1}\} = A}.

	Il caso base si ha per l'insieme \math{X_{i, i + 1}}, ovvero quando
	\math{i + 1 = j}. Infatti, l'insieme costituito da tutte le attività
	che vengono dopo \math{a_{i}} e prima di \math{a_{i + 1}} è, per
	definizione, l'insieme vuoto, perché le due attivitá sono contigue.

	\begin[width = 50%fw]{parbox}
		\begin[mode = display]{math}
			S_{i, i + 1} = \varnothing
		\end{math}
	\end{parbox}
	\begin[width = 50%fw]{parbox}
		\begin[mode = display]{math}
			\mi{Opt}(i, i + 1) = \abs{\varnothing} = 0
		\end{math}
	\end{parbox}
	\par

	Per quanto riguarda il passo ricorsivo, si consideri una generica
	soluzione ottimale \math{S_{i, j}}. Tale soluzione può essere scritta
	come \math{S_{i, k} \cup \{a_{k}\} \cup S_{k, j}}, dove \math{S_{i, k}}
	e \math{S_{k, j}} sono le soluzioni ottimali rispettivamente delle
	istanze \math{X_{i, k}} e \math{X_{k, j}} e dove \math{a_{k}} è il
	valore che permette di restituire tali soluzioni ottimali.

	\begin[width = 50%fw]{parbox}
		\begin[mode = display]{math}
			S_{i, j} = \mi{max}\{S_{i, k} \cup \{a_{k}\} \cup S_{k, j} | i < k < j\}
		\end{math}
	\end{parbox}
	\begin[width = 50%fw]{parbox}
		\begin[mode = display]{math}
			\mi{Opt}(i, j) = \mi{max}\{\mi{Opt}(i, k) + 1 + \mi{Opt}(k, j) | i < k < j\}
		\end{math}
	\end{parbox}
	\par

	È facile verificare che un algoritmo di programmazione dinamica basato
	su tale equazione avrebbe un tempo di esecuzione (almeno) quadratico.
	Questo perché per ottenere la soluzione ottimale per una intera istanza
	è necessario calcolare le soluzioni ottimali per tutte le istanze
	\math{X_{a, b}} con \math{0 \leq a \leq n} e \math{0 \leq b \leq n}.

	Il problema Interval Scheduling può essere risolto in maniera più
	semplice e più efficiente applicando la tecnica greedy. Dato un
	insieme \math{A} di attività, l'algoritmo greedy procede come segue:

	\begin[width = 50%fw]{parbox}
		\begin{verbatim}
			procedure INTERVAL-SCHEDULING(A)
			1    A \unichar{U+2190} A ordinato per tempo di fine non descrescente
			2    X \unichar{U+2190} \{a\textsubscript{1}\}
			3    k \unichar{U+2190} 1

			     \bigskip
			5    for i \unichar{U+2190} 2 to |A| do
			6        if s\textsubscript{i} \unichar{U+2265} f\textsubscript{k} then
			7            X \unichar{U+2190} \{a\textsubscript{s}\} \unichar{U+222A} X
			8            k \unichar{U+2190} i

			     \bigskip
			9    return X
		\end{verbatim}
	\end{parbox}
	\begin[width = 50%fw]{parbox}
		\begin{enumerate}
			\begin{item}
				Gli elementi di \math{A} vengono ordinati
				per tempo di fine non decrescente;
			\end{item}
			\begin{item}
				Viene creato l'insieme soluzione \math{X}, che
				inizialmente contiene soltanto \math{a_{1}};
			\end{item}
			\begin{item}
				Sia \math{a_{k}} l'ultimo elemento dell'insieme
				\math{X}, ovvero l'elemento di \math{X} avente
				tempo di fine maggiore. Se l'\math{i}-esimo elemento
				di \math{A} é compatibile con \math{a_{k}}, allora
				viene aggiunto ad \math{X} diventando il nuovo
				\math{a_{k}};
			\end{item}
			\begin{item}
				Se tutte le attività di \math{A} sono state
				considerate l'algoritmo termina, altrimenti
				si riprende dal punto precedente.
			\end{item}
		\end{enumerate}
	\end{parbox}
	\par\bigskip

	Tale algoritmo è effettivamente un algoritmo greedy, perché la scelta
	localmente ottima viene considerata anche globalmente ottima. Tuttavia,
	affinché questo possa considerarsi corretto, occorre dimostrare la
	validità della proprietà della scelta greedy, ovvero che l'attività che
	viene per prima in una certa iterazione (soluzione ottimale locale) è
	effettivamente membro della soluzione ottimale globale. A dire il vero,
	dato che ad ogni iterazione l'insieme \math{A} viene ridotto e l'attività
	che viene considerata è sempre la prima, per dimostrare la validità
	della proprietà della scelta greedy è sufficiente dimostrare che
	l'attività \math{a_{1}}, quella che nell'ordine temporale viene
	prima di tutte, è sempre parte della soluzione ottima.

	\begin{theorem}
		\strong{Proprietà della scelta greedy per il problema Interval
		Scheduling}. Sia dato un insieme di attività \math{A = \langle
		a_{1}, a_{2}, \unicodeellipsis, a_{n} \rangle} ordinate per
		tempo di fine non decrescente. L'attività \math{a_{1}} è
		sempre parte della soluzione ottima.

		\bigskip
		\strong{Dimostrazione}. Sia \math{X} la soluzione ottima
		per l'insieme \math{A}. Sia poi \math{{a'}_{1}} l'attività
		in \math{X} avente minor tempo di fine. Naturalmente, se
		\math{{a'}_{1}} coincide con \math{a_{1}}, allora la
		dimostrazione è terminata; altrimenti, si sostituisca in
		\math{X} l'attività \math{{a'}_{1}} con \math{a_{1}}. Il
		nuovo insieme \math{X} ha mantenuto la stessa cardinalità
		ed è ancora costituito da attività mutualmente compatibili,
		pertanto il nuovo \math{X} è un sottoinsieme massimo di
		attività compatibili di \math{A} che include ora \math{a_{1}}.
	\end{theorem}

	Si noti come l'algoritmo che risolve il problema Interval Scheduling 
	applicando la tecnica greedy è nettamente più veloce di quello che lo
	risolve applicando la programmazione dinamica. Infatti, indicando con
	\math{n} la cardinalità di \math{A}, si ha che il loop principale
	(righe 5-8) esegue esattamente \math{\abs{n}} volte un blocco di
	codice con tempo di esecuzione unitario, mentre l'ordinamento di
	\math{A} avviene (assumendo di usare un algoritmo di ordinamento
	efficiente) in tempo \math{O(n \mi{log}(n))}. Pertanto, asintoticamente,
	il tempo di esecuzione complessivo viene ad essere
	\math{O(n \mi{log}(n)) + O(n) = O(n \mi{log}(n))}.

\end{sile}
